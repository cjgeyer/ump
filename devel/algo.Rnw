
\documentclass{article}

\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{ifthen}
\usepackage{indentfirst}
\usepackage{natbib}
\usepackage[colorlinks=true,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{url}
\usepackage{doi}

\newcommand{\set}[1]{\{\, #1 \,\}}
\newcommand{\inner}[1]{\langle #1 \rangle}

\newcommand{\opand}{\mathrel{\rm and}}
\newcommand{\opor}{\mathrel{\rm or}}

\DeclareMathOperator{\pr}{Pr}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\logit}{logit}

\newcommand{\BinomialDis}{\text{Bin}}
\newcommand{\NegativeBinomialDis}{\text{NegBin}}
\newcommand{\PoissonDis}{\text{Pois}}

\newcommand{\fatdot}{\,\cdot\,}

\newcommand{\reals}{\mathbb{R}}

\let\code=\texttt

\newcommand{\REVISED}{\begin{center} \LARGE REVISED DOWN TO HERE \end{center}}
\newcommand{\MOVED}[1][equation]{\begin{center} [#1 moved] \end{center}}

\RequirePackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

% \VignetteIndexEntry{UMP Package Design Document}

\begin{document}

\title{Algorithm for UMP and UMPU Tests in Some Discrete Exponential Families}

\author{Charles J. Geyer}

\maketitle

\section{Introduction}

\subsection{Background}

\cite{geyer-meeden} introduced the idea of fuzzy (also called abstract
randomized) hypothesis tests and confidence intervals.  Their theory
is a reinterpretation of the classical theory of
uniformly most powerful (UMP) one-tailed tests and
the uniformly most powerful unbiased (UMPU) two-tailed tests,
which has been accepted as a key part of the theory of hypothesis tests
since it was introduced by Neyman and Pearson in the 1930's.

Those hypothesis tests (or the confidence intervals that are dual to them)
are randomized, that is, they involve additional artificial randomization
independent of the randomness in the data.  So two statisticians can
analyze exactly the same data by exactly the same procedure, and arrive
at opposite decisions because of the artificial randomization.  Consequently,
these procedures are not widely used in applied statistics.

\cite{geyer-meeden} argue that this can be fixed by removing the artificial
randomness.  They bring in the distinction between a random variable, which
is a theoretical construct described by a probability distribution, and
a \emph{realization} of the random variable, which is just a number
purportedly obtained from some random process described by that
probability distribution.  Classical UMP and UMPU tests (and the confidence
intervals dual to them) use realized artificial randomness.  That is why
different statisticians get different results for the same data and same
procedure.  \cite{geyer-meeden} just say no.  Leave the
randomization abstract.  Just describe the distribution of the random
variable, and leave it at that.  If users really want realizations,
then they can simulate such themselves.

So \cite{geyer-meeden} in no way disagree with the classical theory
of UMP and UMPU tests.  They only think we should leave the randomization
abstract, so the results of any analysis are unique, and all analysts agree
(on the abstract random variable described by a probability distribution
that is the result).

\subsection{Fuzzy Procedures}

A classical randomized test rejects the null hypothesis with probability
given by a \emph{critical function} $\phi$.  When the observed data is
$x$, then we reject the null with probability $\phi(x)$.
But, of course, it also depends on the
significance level $\alpha$ and the hypothesized value $\theta$ of
the parameter
under the null hypothesis.  So we, following \citet{geyer-meeden} write
it $\phi(x, \alpha, \theta)$.  By making the test \emph{abstract randomized}
\citet{geyer-meeden} mean: describe this function $\phi$ and stop there.

They call these procedures \emph{fuzzy} with the same
meaning as in fuzzy set theory \citep{fuzz}.
A set in ordinary mathematics can be described by its
\emph{indicator function} which is zero-or-one-valued.
A \emph{fuzzy} set in fuzzy set theory can be described by its
\emph{membership function} which takes values in the closed interval
$[0, 1]$.  Values strictly between zero and one correspond to points
we are unsure whether or not they are in the fuzzy set and the value
of the membership function says how much we think they are in or out.

Statisticians and others familiar with probability theory will note
that probability is also given by numbers between zero and one,
and probability also is thought to describe uncertainty, so is fuzzy set theory
just probability theory under another name?  No.  The operations
of fuzzy set theory \citep{fuzz} have nothing to do with probability
theory.  It is really different.  But \citep{geyer-meeden} do not use
any of those operations.  They just use some of the terminology,
mainly membership functions.  They also use the term \emph{crisp}.

A fuzzy set is \emph{crisp} if its membership function is zero-or-one
valued (so we are certain about which points are in the set and which
are not, and this is just like the indicator function of a set in
ordinary mathematics).  So crisp is just the fuzzy set theory way
of saying ordinary.

So classical confidence intervals (which are ordinary sets) are
crisp fuzzy sets described by zero-or-one-valued membership functions.
To have abstract randomized confidence intervals (dual to classical
UMP and UMPU tests) \citet{geyer-meeden} need general fuzzy sets
described by membership functions that take values in the closed
interval $[0, 1]$.

\citet{geyer-meeden} say the following about the critical function.
\begin{itemize}
\item The function $\phi(\fatdot, \alpha, \theta)$ they call the
    fuzzy (or abstract randomized) decision function for the hypothesis test
    having significance level $\alpha$ and null hypothesis $\theta$.
\item The function $1 - \phi(x, \alpha, \fatdot)$ they call
    the (membership function in the sense of fuzzy set theory) of the
    fuzzy confidence interval with coverage $1 - \alpha$ and observed data $x$.
\item The function $\phi(x, \fatdot, \theta)$ they call the
    (distribution function of) the abstract randomized (or fuzzy) $P$-value
    for the hypothesis test having null hypothesis $\theta$ and
    observed data $x$.
\end{itemize}

That the function $\phi(x, \fatdot, \theta)$ is a distribution
function is Theorem~\ref{th:fuzzy-df} below.

That the function $1 - \phi(x, \alpha, \fatdot)$ is a membership
function is obvious.  If $\phi$ takes values in $[0, 1]$.
then so does $1 - \phi$.

\citet{geyer-meeden} only discuss fuzzy hypothesis tests and confidence
intervals based on UMP and UMPU theory.  Their discussants discuss other
kinds.  Papers citing
\citet{geyer-meeden} discuss other kinds.  Every randomized test
can be described by a critical function as described above, and
then we have the interpretations listed above.

For UMP and UMPU fuzzy confidence intervals, we can also prove
that they are convex in the sense of fuzzy set theory
(Theorem~\ref{th:fuzzy-convex} below).

\subsection{More on Interpretation}

For a one-tailed test, including UMP, the null and alternative hypotheses are
\begin{subequations}
\begin{gather}
   H_0 : \text{true unknown parameter} \le \theta
   \\
   H_1 : \text{true unknown parameter} > \theta
\end{gather}
\end{subequations}
for an upper-tailed test, and the same with the inequalities reversed for
a lower-tailed test.  The $\theta$ in these equations is the $\theta$
in the critical function $\phi(x, \alpha, \theta)$.

For a two-tailed test, including UMPU, the null and alternative hypotheses are
\begin{subequations}
\begin{gather}
   H_0 : \text{true unknown parameter} = \theta
   \\
   H_1 : \text{true unknown parameter} \neq \theta
\end{gather}
\end{subequations}
and the $\theta$ in these equations is the $\theta$
in the critical function $\phi(x, \alpha, \theta)$.

The power function of the test is
$$
   p(\theta', \theta) = E_{\theta'}\{ \phi(X, \alpha, \theta) \}
$$
where $\theta'$ varies over the parameter space and $\theta$ is fixed
at the value hypothesized under the null hypothesis.

\begin{itemize}
\item The \emph{exactness} property is $p(\theta, \theta) = \alpha$
    for all $\theta$.
\item The \emph{unbiasedness} property is $p(\theta', \theta) \ge \alpha$,
    for all $\theta'$ and $\theta$.
\item The \emph{UMP} property
    is $p(\theta', \theta) \ge \tilde{p}(\theta', \theta)$
    whenever $\theta' < \theta$ for a lower-tailed test,
    whenever $\theta' > \theta$ for an upper-tailed test, and
    whenever $\theta' \neq \theta$ for a two-tailed test,
    and all power functions $\tilde{p}$ of other tests
    in the class of interest (satisfying exactness
    for UMP and satisfying exactness and unbiasedness for UMPU).
\end{itemize}

When the data are discrete, only a randomized test can have the exactness
property (that's why randomized tests were invented by Neyman and Pearson).

The interpretation of the fuzzy $P$-value is simplest.  The function
$\phi(x, \fatdot, \theta)$ is a distribution function.  Let $P$ denote
a random variable that has this distribution.  Then the test that rejects
the null when $P \le \alpha$ is the classical UMP or UMPU test.
All \citet{geyer-meeden} are saying in this case is report
the \emph{distribution of $P$} and stop there rather than going on to
generating a number that is purported to be a realization of $P$ or
comparing that to $\alpha$ and making a decision.
They claim that that distribution (the fuzzy $P$-value) is just as easy
to interpret as a classical (crisp) $P$-value if one properly takes into
account the equivocalness of $P$-values of intermediate size (near $0.05$
by convention).

The value of thinking of a fuzzy confidence interval as a fuzzy set comes
from the partial credit interpretation.  As good frequentists, we know that
the performance of a confidence interval is measured by averaging over all
possible data.  The exactness property is
$$
   E_\theta\{ 1 - \phi(X, \alpha, \theta) \} =  1 - \alpha,
   \qquad \text{for all $\theta$}.
$$
in which we see that when $\phi(x, \alpha, \theta)$ is strictly between
zero and one, the expectation gives partial credit for coverage.

Define $c(\theta', \theta) = 1 - p(\theta', \theta)$.  Call it the coverage
function for the fuzzy confidence interval.  It is the probability that the
interval covers $\theta$ when $\theta'$ is the true unknown parameter value.
Then we have the same three properties.  Here we only consider UMPU two-tailed
intervals.
\begin{itemize}
\item The \emph{exactness} property is $c(\theta, \theta) = 1 - \alpha$
    for all $\theta$.  We have the exact desired coverage, unlike crisp
    confidence intervals.
\item The \emph{unbiasedness} property is $c(\theta', \theta) \le 1 - \alpha$,
    for all $\theta'$ and $\theta$.
    The fuzzy confidence interval has a higher probability of covering
    the true unknown parameter value than any other parameter value.
\item The UMP property
    is $c(\theta', \theta) \le \tilde{c}(\theta', \theta)$
    for all $\theta'$ and $\theta$ where $\tilde{c}$ is the coverage function
    for any other fuzzy confidence interval satisfying
    exactness and unbiasedness.  The fuzzy confidence interval dual to the
    UMPU test has less coverage for any false parameter value than any
    other fuzzy confidence interval.
\end{itemize}

\subsection{Computing}

So everything depends on the critical function $\phi(x, \alpha, \theta)$.
We want to provide computation of the critical function for
some discrete exponential families that arise commonly in statistical
inference (Section~\ref{sec:families} below).
See Section~\ref{sec:ump-umpu} below for more on UMP and UMPU procedures
and critical functions.  For more, see \citet{geyer-meeden}.
For even more, see \citet{lehmann}.

We want our implementation of the critical function to efficiently
vectorize over a vector of $\alpha$ values for computation of fuzzy $P$-values
and vectorize over a vector of $\theta$ values for computation of fuzzy
confidence intervals.  More about computation starting
with Section~\ref{sec:algorithms} below.

\pagebreak[3]

\subsection{Families of Distributions} \label{sec:families}

The discrete exponential families we are interested in are
\begin{itemize}
\item the binomial distribution (more strictly, any binomial exponential
    family of distributions --- different sample sizes determine different
    exponential families), needed not only for binomial data but also
    for comparison of the distributions of two independent Poisson random
    variables, two components of a multinomial random vector, or the
    UMP and UMPU competitors of McNemar's test
    (Sections~\ref{sec:model-binomial}, \ref{sec:two-poisson-indep},
    \ref{sec:different-answers-same-question},
    and~\ref{sec:answers-different-questions} below),
\item the Poisson distribution (more strictly, the Poisson exponential
    family of distributions), needed for Poisson data
    (Section~\ref{sec:model-poisson} below),
\item the negative binomial distribution (more strictly, any negative
    binomial exponential family of distributions --- the shape
    parameter is considered known (otherwise we would not have an
    exponential family) so different shape parameters
    determine different exponential families),
    needed for negative binomial data (Section~\ref{sec:negbin} below),
\item Fisher's noncentral hypergeometric distribution (more strictly,
    any exponential family generated by a hypergeometric distribution ---
    different numbers of population successes, population failures, and
    sample sizes determine
    different hypergeometric distributions, hence different exponential
    families),
    needed for comparison of the distributions of two independent binomial
    random variables or the UMP and UMPU competitors of Fisher's exact test
    (Sections~\ref{sec:two-binom-indep} and~\ref{sec:exact} below),
    and
\item any exponential family generated by a negative hypergeometric
    distribution, needed for comparison of the distributions of two independent
    negative binomial random variates
    (Section~\ref{sec:two-negbin-indep} below).
\end{itemize}

The first three of these are well-known and well supported in R with the
usual quartet of d, p, q, and r functions (like \code{dbinom}, \code{pbinom},
\code{qbinom}, and \code{rbinom}).
The last two of these do not have full R support (even in a CRAN package).
The next to last does have some literature \citep{liao-rosen} about algorithms
for it and is implemented in R package \code{MCMCpack} \citep{MCMCpack-package}
but only d and r functions
(\code{dnoncenhypergeom} and \code{rnoncenhypergeom}),
which are insufficient for UMP and UMPU calculation
(which need p and q functions too).
The last does not even have that
(neither literature nor R implementation).

\subsection{More on Noncentral Hypergeometric}
\label{sec:noncentral-hypergeometric}

The hypergeometric distribution \citep{hypergeometric-distribution}
is the distribution of the number of successes
in sampling without replacement from a finite population
(that is, just like the binomial distribution except for sampling
without replacement).
The probability mass function is
$$
   f(x) = \frac{\binom{K}{x} \binom{N - K}{n - x}}{\binom{N}{n}}
$$
where there are $K$ successes for $N$ individuals in the population
and $x$ successes for $n$ individuals in the sample.
The support of this distribution is a bit tricky.
A binomial coefficient $\binom{m}{k}$ must satisfy
$0 \le k \le m$, otherwise it is zero,
and we have three binomial coefficients to deal with,
so we must have
\begin{gather*}
   0 \le x \le K
   \\
   0 \le n - x \le N - K
   \\
   0 \le n \le N
\end{gather*}
Hence in order for the distribution to exist we must have
$$
   0 \le n \le N
$$
and in order for $x$ to be in the support of the distribution we must have
$$
   \max(0, n - (N - K)) \le x \le \min(K, n)
$$

The exponential family generated by this distribution, sometimes
called Fisher's noncentral hypergeometric distribution
\citep{fisher,cornfield,agresti,liao-rosen,
noncentral-hypergeometric-distribution} has probability mass function
$$
   f_\theta(x) =
   \frac{e^{x \theta} \binom{K}{x} \binom{N - K}{n - x}}{c(\theta)}
$$
where
$$
   c(\theta) = \sum_{x = \max(0, n + K - N)}^{\min(K, n)}
   e^{x \theta} \binom{K}{x} \binom{N - K}{n - x}
$$
where $x$ is the canonical statistic, $\theta$ is the canonical parameter,
and $c$ is the Laplace transform of the hypergeometric distribution.
% check Mathematica
% Sum[ Exp[x theta] Binomial[n1, x] Binomial[n2, n - x],
%     {x, Max[0, n - n2], Min[n1, n]},
%     Assumptions -> n >= 0 && n <= n1 + n2 && x >= 0 &&
%     x >= n - n2 && x <= n1 && x <= n ]
% Needs HypergeometricPFQ which R package gsl does not supply

\subsection{More on Noncentral Negative Hypergeometric}
\label{sec:noncentral-negative-hypergeometric}

The negative hypergeometric distribution
\citep{negative-hypergeometric-distribution}
is like the hypergeometric distribution except with inverse sampling.
(Like the negative binomial distribution, the random variable of interest
is the number of successes before the $r$-th failure, but like the
hypergeometric distribution sampling is without replacement.)
The probability mass function is
$$
   f(x) = \frac{\binom{x + r - 1}{x} \binom{N - r - x}{K - x}}
   {\binom{N}{K}}
$$
where there are $K$ successes for $N$ individuals in the population
and $x$ successes and $r$ failures in the sample.

We have three binomial coefficients to deal with,
so we must have
\begin{gather*}
   0 \le x \le x + r - 1
   \\
   0 \le K - x \le N - r - x
   \\
   0 \le K \le N
\end{gather*}
Hence in order for the distribution to exist we must have
$$
   0 \le K \le N \opand 1 \le r \le N - K
$$
and in order for $x$ to be in the support of the distribution we must have
$$
   0 \le x \le K
$$

The exponential family generated by this distribution
has probability mass function
\begin{equation} \label{eq:noncentral-negative-hypergeometric-pmf}
   f_\theta(x) =
   \frac{e^{x \theta} \binom{x + r - 1}{x} \binom{N - r - x}{K - x}}{c(\theta)}
\end{equation}
where
$$
   c(\theta) = \sum_{x = 0}^K
   e^{x \theta} \binom{x + r - 1}{x} \binom{N - r - x}{K - x}
$$
where $x$ is the canonical statistic, $\theta$ is the canonical parameter,
and $c$ is the Laplace transform of the negative hypergeometric distribution.

\subsection{UMP and UMPU} \label{sec:ump-umpu}

The following is mostly taken from \citet{geyer-meeden}.

\subsubsection{Significance Level Zero or One}

The only test that has significance level zero must reject the null hypothesis
almost surely, hence for all possible values of a discrete test statistic.
Thus we define
\begin{equation} \label{eq:alpha-zero}
   \phi(x, 0, \theta) = 0, \qquad \text{for all $x$ and $\theta$}.
\end{equation}
Similarly
\begin{equation} \label{eq:alpha-one}
   \phi(x, 1, \theta) = 1, \qquad \text{for all $x$ and $\theta$}.
\end{equation}
In what follows, we need \eqref{eq:alpha-zero} because that will
not follow from the equations we use when $\alpha > 0$ when the
sample space is infinite (although it will be a limit of those equations).
In contrast, we will not need \eqref{eq:alpha-one} because that will
be a special case of the equations we use when $\alpha > 0$.

\subsubsection{UMP}

Lehmann (1959, pp.~68--69) says for a one-parameter
model with likelihood ratio monotone in the statistic $T(X)$
there exists a UMP test having null and alternative hypotheses
\begin{gather*}
   H_0 : \text{true unknown parameter} \le \theta
   \\
   H_1 : \text{true unknown parameter} > \theta
\end{gather*}
and critical function $\phi$ for size $\alpha$ defined by
\begin{equation} \label{eq:ump-crit}
   \phi(x, \alpha, \theta)
   =
   \begin{cases}
   1, & T(x) > C
   \\
   \gamma, & T(x) = C
   \\
   0, & T(x) < C
   \end{cases}
\end{equation}
where the constants $\gamma$ and $C$ are determined by
$$
   E_\theta\{\phi(X, \alpha, \theta)\} = \alpha.
$$
The description of the analogous lower-tailed test is
the same except that all inequalities are reversed.

\subsubsection{UMPU} \label{sec:umpu}

Lehmann (1959, pp.~126--127) says for a one-parameter
regular full exponential family model with canonical statistic $T(X)$ 
and canonical parameter $\theta$
there exists a UMPU test having null and alternative hypotheses
\begin{gather*}
   H_0 : \text{true unknown parameter} = \theta
   \\
   H_1 : \text{true unknown parameter} \neq \theta
\end{gather*}
and critical function $\phi$ for size $\alpha$ defined by
\begin{equation} \label{eq:umpu-crit}
   \phi(x, \alpha, \theta)
   =
   \begin{cases}
   1, & T(x) < C_1
   \\
   \gamma_1, & T(x) = C_1
   \\
   0, & C_1 < T(x) < C_2
   \\
   \gamma_2, & T(x) = C_2
   \\
   1, & C_2 < T(x)
   \end{cases}
\end{equation}
where $C_1 \le C_2$ and the constants $\gamma_1$, $\gamma_2$, $C_1$, and $C_2$
are determined by
\begin{subequations}
\begin{align}
   E_\theta\{\phi(X, \alpha, \theta)\} & = \alpha
   \label{eq:umpu-a}
   \\
   E_\theta\{T(X) \phi(X, \alpha, \theta)\} & = \alpha E_\theta\{T(X)\}
   \label{eq:umpu-b}
\end{align}
\end{subequations}
That both sides of \eqref{eq:umpu-b} are finite is guaranteed by the
regular full exponential family assumption.

\subsubsection{Two-Point Sample Space} \label{sec:two-point}

In case the sample space contains only two points,
without loss of generality, we take $T(X)$ to have a Bernoulli
distribution so $C_1 = 0$ and $C_2 = 1$.  Let $p = \pr\{T(X) = 1\}$.
Then \eqref{eq:umpu-a} and \eqref{eq:umpu-b} become
\begin{subequations}
\begin{align}
   \gamma_1 (1 - p) + \gamma_2 p & = \alpha
   \label{eq:umpu-a-two-point}
   \\
   \gamma_2 p & = \alpha p
   \label{eq:umpu-b-two-point}
\end{align}
\end{subequations}
which have solution $\gamma_1 = \gamma_2 = \alpha$.  So in this case
the UMPU test ignores both data and the hypothesized value $p$ under the
null hypothesis: the test rejects the null hypothesis with probability $\alpha$
regardless of either of them.

For any randomized test to be unbiased we must have
$$
   \gamma_1 (1 - p) + \gamma_2 p \ge \alpha
$$
for all $p$ and, together with \eqref{eq:umpu-a-two-point} holding
when $p$ is the value hypothesized under the null hypothesis, this
clearly implies $\gamma_1 = \gamma_2 = \alpha$,
so there is only one unbiased test, and it is, of course, UMP within
the class consisting of that single test.

We only paid this case special attention because, if we don't know about it,
then it seems odd.

\section{Calculating UMP and UMPU}

\subsection{UMP}

In \eqref{eq:ump-crit} the constant $C$
is clearly any $(1 - \alpha)$-th quantile of the distribution
of $T(X)$ for the parameter value $\theta$.
If the event $T(X) = C$ has probability zero, then
the test is effectively not randomized and the value of $\gamma$ is
irrelevant (can be chosen arbitrarily).  Otherwise
\begin{equation} \label{eq:gamma-ump}
   \gamma = \frac{\alpha - \pr_\theta\{T(X) > C\}}{\pr_\theta\{T(X) = C\}}.
\end{equation}

By definition of $(1 - \alpha)$-th quantile
$$
   \pr_\theta\{T(X) > C\} \le \alpha \le \pr_\theta\{T(X) \ge C\}
$$
so \eqref{eq:gamma-ump} is always between zero and one (inclusive).

\subsection{UMPU}

In \eqref{eq:umpu-crit}, if $C_1 = C_2 = C$,
then $\gamma_1 = \gamma_2 = \gamma$ also.
This occurs only in a very special case.
Define
\begin{subequations}
\begin{align}
   p & = \pr_\theta\{T(X) = C\}
   \label{eq:p}
   \\
   \mu & = E_\theta\{T(X)\}
   \label{eq:mu}
\end{align}
\end{subequations}
Then in order to satisfy \eqref{eq:umpu-a} and \eqref{eq:umpu-b} we must have
\begin{align*}
   1 - (1 - \gamma) p & = \alpha
   \\
   \mu - C (1 - \gamma) p & = \alpha \mu
\end{align*}
which solved for $\gamma$ and $C$ gives
\begin{subequations}
\begin{align}
   \gamma & = 1 - \frac{1 - \alpha}{p}
   \label{eq:umpu-spec-a}
   \\
   C & = \mu
   \label{eq:umpu-spec-b}
\end{align}
\end{subequations}
Thus this special case occurs only when $\Pr_\theta\{T(X) = \mu\}$ is
nonzero, and then only for
very large significance levels: $\alpha \ge 1 - p$.
Hence this special case is of no practical importance,
although
it is of some computational importance to get every case right, no weird
bogus results or crashes in unusual special cases.

Returning to the general case, assume for a second that we have
particular $C_1$ and $C_2$ that work for some $x$, $\alpha$, and $\theta$.
(We will see how to determine $C_1$ and $C_2$ presently.)
With $\mu$ still defined by \eqref{eq:mu} and with the definitions
\begin{subequations}
\begin{align}
   p_i & = \pr_\theta\{ T(X) = C_i \}, \qquad i = 1, 2
   \label{eq:pi}
   \\
   P_1 & = \pr_\theta\{ T(X) < C_1 \}
   \label{eq:P1}
   \\
   P_2 & = \pr_\theta\{ T(X) > C_2 \}
   \label{eq:P2}
   \\
   M_1 & = E_\theta\{ T(X) I_{(- \infty, C_1)}[T(X)] \}
   \label{eq:M1}
   \\
   M_2 & = E_\theta\{ T(X) I_{(C_2, \infty)}[T(X)] \}
   \label{eq:M2}
\end{align}
\end{subequations}
Then \eqref{eq:umpu-a} and \eqref{eq:umpu-b} become
\begin{subequations}
\begin{align}
   P_1 + \gamma_1 p_1 + \gamma_2 p_2 + P_2
   & = \alpha
   \label{eq:umpu-a-general}
   \\
   M_1 +  \gamma_1 C_1 p_1 + \gamma_2 C_2 p_2 + M_2
   & = \alpha \mu
   \label{eq:umpu-b-general}
\end{align}
\end{subequations}
which solved for $\gamma_1$ and $\gamma_2$ give
\begin{subequations}
\begin{align}
   \gamma_1
   & =
   \frac{\alpha (C_2 - \mu) + (M_1 - C_2 P_1) + (M_2 - C_2 P_2)}
   {p_1 (C_2 - C_1)}
   \label{eq:gamma-1}
   \\
   \gamma_2
   & =
   \frac{\alpha (\mu - C_1) - (M_2 - C_1 P_2) - (M_1 - C_1 P_1)}
   {p_2 (C_2 - C_1)}
   \label{eq:gamma-2}
\end{align}
\end{subequations}
%
% Solve[ { P1 + gamma1 p1 + gamma2 p2 + P2 == alpha,
%     M1 + gamma1 C1 p1 + gamma2 C2 p2 + M2 == mu alpha }, { gamma1, gamma2 } ]
%
% gamma1 = (alpha (C2 - mu) + (M1 - C2 P1) + (M2 - C2 P2)) / (p1 (C2 - C1))
% gamma2 = (alpha (mu - C1) - (M1 - C1 P1) - (M2 - C1 P2)) / (p2 (C2 - C1))
%
% P1 + gamma1 p1 + gamma2 p2 + P2
% M1 + gamma1 C1 p1 + gamma2 C2 p2 + M2
%
% Solve[ 1 == (alpha (C2 - mu) + (M1 - C2 P1) + (M2 - C2 P2)) / (p1 (C2 - C1)),
%     alpha ]
% Solve[ 1 == (alpha (mu - C1) - (M1 - C1 P1) - (M2 - C1 P2)) / (p2 (C2 - C1)),
%     alpha ]
% Solve[ 0 == (alpha (C2 - mu) + (M1 - C2 P1) + (M2 - C2 P2)) / (p1 (C2 - C1)),
%     alpha ]
% Solve[ 0 == (alpha (mu - C1) - (M1 - C1 P1) - (M2 - C1 P2)) / (p2 (C2 - C1)),
%     alpha ]
%

If \eqref{eq:gamma-1} and \eqref{eq:gamma-2} are both between 0 and 1
(inclusive), then $C_1$ and $C_2$ have been correctly determined.
So this gives us (implicitly) an algorithm: keep trying different
$C_1$ and $C_2$ until
\eqref{eq:gamma-1} and \eqref{eq:gamma-2} both compute numbers between 0 and 1
(inclusive).
More sophisticated algorithms will be developed below.

\subsection{Alternative Formulas for UMPU}

\citet{geyer-meeden} give alternative formulas for $\gamma_1$ and $\gamma_2$.
Define
\begin{subequations}
\begin{align}
   P_{1 2} & = \pr_\theta\{ C_1 < T(X) < C_2 \}
   \label{eq:p12}
   \\
   M_{1 2} & = E_\theta\{ T(X) I_{(C_1, C_2)}[T(X)] \}
   \label{eq:m12}
\end{align}
\end{subequations}
Then
\begin{subequations}
\begin{align}
   1 - \gamma_1
   & =
   \frac{(1 - \alpha) (C_2 - \mu) + M_{1 2} - C_2 P_{1 2}}{p_1 (C_2 - C_1)}
   \label{eq:gamma-1-alt}
   \\
   1 - \gamma_2
   & =
   \frac{(1 - \alpha) (\mu - C_1) - M_{1 2} + C_1 P_{1 2}}{p_2 (C_2 - C_1)}
   \label{eq:gamma-2-alt}
\end{align}
\end{subequations}
where $p_1$ and $p_2$ are still given by \eqref{eq:pi}.

These equations are useful in some theoretical contexts, especially
in the case where $C_1$ and $C_2$ are adjacent atoms
so $M_{1 2} = P_{1 2} = 0$.

It might be thought that these equations are also computationally useful,
avoiding catastrophic cancellation when their values are near zero.
But catastrophic cancellation is only avoided if $M_{1 2}$ and $P_{1 2}$
can be computed without catastrophic cancellation, which the naive method
(subtraction of cumulative distribution function values) fails to do.

Subtracting $\mu$ times \eqref{eq:cond-two-a} from \eqref{eq:cond-two-b} gives
$$
   E_\theta\{ (X - \mu) \phi(X, \alpha, \theta) \} = 0
$$
and this can be rewritten
\begin{equation} \label{eq:cond-two-c}
   E_\theta\{ (\mu - X) I_{(- \infty, \mu)}(X) \phi(X, \alpha, \theta) \}
   =
   E_\theta\{ (X - \mu) I_{(\mu, \infty)}(X) \phi(X, \alpha, \theta) \}
\end{equation}
and this can be used together with either
of \eqref{eq:cond-two-a} or \eqref{eq:cond-two-b} to determine
the UMPU critical function $\phi$.

Since the expectations in \eqref{eq:cond-two-c} are not easy to compute,
\eqref{eq:cond-two-c} may not be computationally helpful.
It does show that UMPU tests are equal
tailed, not in the usual sense of equal probabilities in each tail but
rather in the sense of equal contributions (with opposite sign) to the
expectation of $X - \mu$ in each tail.

\section{Theorems about UMP and UMPU}

\subsection{Fuzzy $\boldsymbol{P}$-Values}

\subsubsection{Continuity, Monotonicity, Piecewise Linearity}

A random variable or its distribution is discrete in the sense
of probability theory if it has countable support, in which case
the smallest support (event having probability one) is the set
of \emph{atoms} (points having positive probability), and we will
call that \emph{the support}.

We will say that a (probabilistically) discrete random variable
or its distribution
is \emph{order discrete} if the support, considered as a subspace
of the real line, is a discrete ordered set: for every point $x$ of the
support except the least there is a next lower point in the support,
and for every point $x$ of the support except the greatest there is
a next higher point in the support.
This property
holds for all of our distributions of interest (which are integer valued).

It holds for all discrete distributions of applied statistics as far as I know.
It is easy enough to construct a probabilistically discrete but not
order discrete example.
A mixture of an atom at $-1$ and $1 / X$ where $X$ is Poisson does the job.
No point above $-1$ is next to it.  But I do not know what an example
of such a distribution would be that has a real application.

\begin{theorem} \label{th:fuzzy-df}
Assume a regular full one-parameter exponential family such that the
canonical statistic is probabilistically and order discrete.
For either UMP or UMPU
the function $F_{x, \theta} : \alpha \mapsto \phi(x, \alpha, \theta)$
is continuous, piecewise linear, nondecreasing on the interval $[0, 1]$,
and maps onto the interval $[0, 1]$.
$F_{x, \theta}$ is strictly increasing near $\alpha$ such that $0 < F_{x, \theta}(\alpha) < 1$.
Hence $F_{x, \theta}$ is the distribution function of a continuous
random variable having support that is a subinterval of $[0, 1]$.

For UMP the distribution having this distribution function is uniform on
an interval.
\end{theorem}

In aid of proving this theorem we state an algorithm
(Algorithm~\ref{alg:lots-of-work} below).
\begin{algorithm}
\caption{Computing the UMPU Critical Function}
\label{alg:lots-of-work}
Assume a regular full one-parameter exponential family such that the
canonical statistic is order discrete.
For fixed $\theta$, the following computes the critical function of the
UMPU test $\phi(x, \alpha, \theta)$ for all $x$ and $\alpha > 0$.
\begin{enumerate}
\item Start with $\alpha = 1$.
\begin{enumerate}
\item If $\mu$ given by \eqref{eq:mu} is an atom, then
$\phi(x, \alpha, \theta)$ is given by
\eqref{eq:umpu-crit} with $C_1 = C_2 = \mu$
and $\gamma_1 = \gamma_2 = \gamma$
given by \eqref{eq:p}, \eqref{eq:umpu-spec-a}, and \eqref{eq:umpu-spec-b}
over the range of $\alpha$ such that
\eqref{eq:umpu-spec-a} is between zero and one.
\item If $\mu$ given by \eqref{eq:mu} is not an atom, then choose $C_1$
and $C_2$ to be adjacent atoms such that $C_1 < \mu < C_2$ and
$\phi(x, \alpha, \theta)$ is given by \eqref{eq:umpu-crit}
with $\gamma_1$ and $\gamma_2$ given by
\eqref{eq:pi}, \eqref{eq:P1}, \eqref{eq:P2},
\eqref{eq:M1}, \eqref{eq:M2},
\eqref{eq:gamma-1}, and \eqref{eq:gamma-2}
over the range of $\alpha$ such that both
\eqref{eq:gamma-1} and \eqref{eq:gamma-2} are between zero and one.
\end{enumerate}
\item Start with the lowest $\alpha$ for which $\phi(x, \alpha, \theta)$
was determined in step 1 or a previous iteration of step 2.
At this point, either $\gamma_1$ or $\gamma_2$
is zero (or both are).
\begin{enumerate}
\item If $\gamma_1$ is zero, then decrease $C_1$ to the next point of
the sample space below the current value and set $\gamma_1 = 1$.
\item If $\gamma_2$ is zero, then increase $C_2$ to next point of
the sample space above the current value and set $\gamma_2 = 1$.
\item Now $\phi(x, \alpha, \theta)$ is given by \eqref{eq:umpu-spec-a}
with $\gamma_1$ and $\gamma_2$ given by
\eqref{eq:pi}, \eqref{eq:P1}, \eqref{eq:P2},
\eqref{eq:M1}, \eqref{eq:M2},
\eqref{eq:gamma-1}, and \eqref{eq:gamma-2}
over the range of $\alpha$ such that both
\eqref{eq:gamma-1} and \eqref{eq:gamma-2} are between zero and one.
\end{enumerate}
\item Repeat step 2 until the whole range $0 < \alpha \le 1$ is covered.
\end{enumerate}
\end{algorithm}

Without the order discrete assumption, it could be
that there would be no next point to select in step~2a or~2b of the
algorithm (or even points $C_1$ and $C_2$ to choose in 1b).

In case the sample space is infinite, which among our exponential
families of interest is for Poisson and negative binomial, step~2 of
the algorithm may need to be repeated an infinite number of times,
as it is clear (from the algorithm and the proof of the theorem)
that $C_1$ visits every point of the sample space
between $\mu$ and the lower bound (which could be $- \infty$ but
is finite for all our exponential families of interest) and $C_2$
visits every point of the sample space
between $\mu$ and the upper bound (which is $\infty$ for the Poisson
and negative binomial families).

Thus, strictly speaking, if one considers termination after a finite
number of steps to be part of the definition of algorithm, this is not
an algorithm.  But we will not worry about that.  Just consider it
a theoretical construction rather than a computer algorithm.
(More on this after the proof.)

\begin{proof}[Proof of Theorem~\ref{th:fuzzy-df}]
In the UMP case \eqref{eq:gamma-ump} computes a number strictly between 0 and 1
if and only if $\alpha$ is strictly between $\pr_\theta\{T(X) > C\}$
and $\pr_\theta\{T(X) \ge C\}$.  On this interval,
$\gamma$ is a linear function of $\alpha$ and goes from 0 to 1.
Thus the fuzzy $P$-value has the continuous uniform distribution on the
interval with endpoints $\pr_\theta\{T(X) > C\}$ and $\pr_\theta\{T(X) \ge C\}$,
where $C$ is the observed value of $T(X)$.

For UMPU we apply Algorithm~\ref{alg:lots-of-work}.
What it computes is continuous in $\alpha$ because steps~2a and~2b do
not change the critical function (merely its description) and the other
steps change the critical function continuously in $\alpha$ using
\eqref{eq:umpu-spec-a} in step~1a or 
\eqref{eq:gamma-1} and \eqref{eq:gamma-2} in step~1b or~2c.
Moreover, \eqref{eq:umpu-spec-a},
\eqref{eq:gamma-1}, and \eqref{eq:gamma-2} are linear and strictly
increasing in $\alpha$.  So this proves
$F_{x, \theta}$ is continuous, piecewise linear, and increasing
on the open interval where its values are strictly between 0 and 1.
And we have proved that the supremum of its values is 1 but still
have to prove the infimum is 0.

If we ever have $C_1 < T(x) < C_2$ at any point in execution of
Algorithm~\ref{alg:lots-of-work}, then we have $F_{x, \theta}(\alpha) = 0$
for the rest of the execution of the algorithm.  Thus the only way we can
have $F_{x, \theta}(\alpha) > 0$ for the whole execution is,
if from some point in the execution onward either step~2a or step~2b
is never executed again (hence the other is executed infinitely often).
This can only happen with an infinite sample space.  With the families
of interest, all of which are bounded below, this would mean that $C_1$
is eventually constant for an infinite number of iterations of the algorithm
while $C_2 \to \infty$.  We will do this case; the other case is similar
(just swap left and right on the number line).

By dominated convergence, $C_2 \to \infty$ implies $P_2 \to 0$ and
$M_2 \to 0$ (for a dominating function we can take
$\lvert T(X) \rvert$, which is guaranteed
to be integrable by the model
being a regular full exponential family.
Plugging those limits into \eqref{eq:umpu-a-general}
and \eqref{eq:umpu-b-general} gives
\begin{subequations}
\begin{align}
   P_1 + \gamma_1 p_1 
   & = \alpha
   \label{eq:umpu-limit-up}
   \\
   M_1 +  \gamma_1 C_1 p_1
   & = \alpha \mu
   \label{eq:umpu-limit-up-moo}
\end{align}
\end{subequations}
because $0 \le p_2 \le p_2 + P_2 \to 0$
and eventually (when $C_2$ is positive) $0 \le C_2 p_2 \le C_2 p_2 + M_2 \to 0$
and $0 \le \gamma_2 \le 1$.
But \eqref{eq:umpu-limit-up} and \eqref{eq:umpu-limit-up-moo}
are the equations for a lower-tailed
UMP test, so the solutions are given by \eqref{eq:gamma-ump}
with the inequality reversed
$$
   \gamma_2 = \frac{\alpha - P_1}{p_1}.
$$
Hence for sufficiently small $\alpha$ this will go negative unless $P_1 = 0$
which implies that $C_1$ is the lower bound of the sample space, in which
case $\alpha$ can decrease to zero.  Thus
Algorithm~\ref{alg:lots-of-work} can make $\alpha$ arbitrarily
close to zero, and when it does $F_{x, \theta}(\alpha)$ also gets
arbitrarily close to zero.  This proves the assertions about continuity
on $[0, 1]$.
\end{proof}

As noted before the proof of Theorem~\ref{th:fuzzy-df},
Algorithm~\ref{alg:lots-of-work} is not, strictly speaking, an algorithm
because it can need to do an infinite amount of work.
However, if we are only interested in one fixed $x$ rather than all $x$,
the proof of Theorem~\ref{th:fuzzy-df} shows that we do an infinite amount
of work only if we are doing a UMPU two-tailed test and $T(x)$ is a boundary
point of its sample space.  In that case we will have to use a convergence
tolerance and stop when $\alpha$ gets close enough to zero
in order to have a computer algorithm.
(Theorem~\ref{th:small-alpha} below helps in choosing the convergence
tolerance.)

However, Algorithm~\ref{alg:lots-of-work} still does more work than necessary.
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%

\subsubsection{Endpoint Behavior} \label{sec:endpoint-p}

The behavior of the (distribution function of the) fuzzy $P$-value where
its values are near 1 has already
been described (Step 1 of Algorithm~\ref{alg:lots-of-work}).  In either
case (Step~1a or~1b), it is piecewise linear, continuous, and satisfies
\eqref{eq:alpha-one}.

For any continuous function, we say a point $x$ in its domain is a \emph{knot}
if the first derivative or some higher derivative is discontinuous at $x$.
If the function is given by a formula, the formula is different on each
side of the knot.  The way R draws graphs of functions (connecting the
dots) these discontinuities are smoothed out unless the values at the
knots are included in the input to the plot.

\begin{theorem} \label{th:small-alpha}
With the assumptions of Theorem~\ref{th:fuzzy-df},
in case the canonical statistic $T(x)$ is on the boundary of the sample
space of $T(X)$, which is unbounded (in the other direction),
$$
   \frac{\phi(x, \alpha, \theta)}{\alpha}
   \to \frac{1}{\pr_\theta\{T(X) = T(x)\}},
   \qquad \text{as $\alpha \to 0$}.
$$

In all other cases $\phi(x, \fatdot, \theta)$ is
piecewise linear on $[0, 1]$ with a finite number of knots,
continuous, and satisfies \eqref{eq:alpha-zero}.
\end{theorem}
All of this was proved in the proof of Theorem~\ref{th:fuzzy-df}.

\subsection{Fuzzy Confidence Intervals}

\subsubsection{Some Fundamentals of Exponential Families}
\label{sec:fundamentals}

The probability mass function of the canonical statistic
of a discrete exponential family can be written
\begin{equation} \label{eq:expfam-pmf}
    f_\theta(x) = e^{x \theta - c(\theta)} \lambda(x)
\end{equation}
where $\theta$ is the canonical parameter, $c$ is the cumulant function,
and $\lambda$ is positive when $x$ is an atom and zero otherwise.
For general (not necessarily discrete) families, the formula is similar
\citep[Section~1.2]{geyer-thesis}.

The function $c$ is called the cumulant function of the family.
From probabilities summing to one we get
\begin{equation} \label{eq:cumfun}
   c(\theta) = \log\left( \sum_{x \in \reals} e^{x \theta} \lambda(x) \right)
\end{equation}
where the only terms in the sum that contribute are $x$ such that
$\lambda(x) > 0$, the atoms of the distribution.
We take \eqref{eq:cumfun} to define the cumulant function on the whole
real line, writing $c(\theta) = \infty$ if the sum does not exist.
Then $\theta$ is the canonical parameter of the exponential family, and
\begin{equation} \label{eq:full-canonical-parameter-space}
   \Theta = \set{ \theta \in \reals : c(\theta) < \infty }
\end{equation}
is the canonical parameter space of the \emph{full} exponential family
containing the originally given exponential family
(with the probability mass functions of the family being given by
\eqref{eq:expfam-pmf}).  Cumulant functions are convex
\citep[Theorem~7.1]{barndorff-nielsen}.   So the full canonical parameter
space \eqref{eq:full-canonical-parameter-space} is an interval of
real numbers.  It need not be the whole real line.  For negative binomial
\eqref{eq:full-canonical-parameter-space} is the half-line $(- \infty, 0)$.

The full family is \emph{regular} if
\eqref{eq:full-canonical-parameter-space} is
an open subset of the real numbers.

Derivatives of the cumulant function are cumulants (hence the name).
The first two are
\begin{subequations}
\begin{align}
   c'(\theta) & = E_\theta(X)
   \label{eq:cumfun-first-deriv}
   \\
   c''(\theta) & = \var_\theta(X)
   \label{eq:cumfun-second-deriv}
\end{align}
\end{subequations}
\citep[Theorem~8.1]{barndorff-nielsen}.  These equations hold at all
$\theta$ in the interior of \eqref{eq:full-canonical-parameter-space},
hence, for a \emph{regular} full exponential family,
for all $\theta \in \Theta$.

From \eqref{eq:cumfun-second-deriv} we see that, unless the support
is a single point, $c''(\theta) > 0$, so $c$ is a strictly convex
function \citep[Theorem~7.1]{barndorff-nielsen}.
Hence $c'$ is a strictly increasing on $\Theta$ (assuming regular).
Hence by the inverse function theorem, $c'$ is an invertible function.
So
$$
   \mu = c'(\theta) = E_\theta(X)
$$
is a one-to-one mapping between the canonical parameter $\theta$ and
the mean value parameter $\mu$.  And by moment generating function theory
and the inverse function theorem, the mappings both ways are infinitely
differentiable (a fact we will not use in this document).
Here we only need that the mappings are both continuous (which is implied
by differentiability), so both map an open interval to an open interval.

It is not obvious from what we have said so far, but the mean value
parameter space (the range of the function $c' : \Theta \to \reals$)
is the interior of the convex hull of the support of the canonical statistic
\citep[Theorems~8.1 and~9.2]{barndorff-nielsen}.

Cumulant functions are lower semicontinuous on $\reals$
\citep[Theorem~7.1]{barndorff-nielsen}.  So $c(\theta) = \infty$
implies
$$
   c(\theta_n) \to \infty, \qquad \text{as $\theta_n \to \theta$}.
$$

\subsubsection{Convexity and Continuity}

\begin{lemma} \label{lem:gamma-derivative}
In \eqref{eq:gamma-1} and \eqref{eq:gamma-2}, if $C_1$ and $C_2$ are chosen
so that $C_1 \neq C_2$ and both $\gamma_1$ and $\gamma_2$ are strictly
between zero and one (so these formulas define the critical function of a
UMPU test), and if the support of the canonical statistic has more than two
points (excluding the case described in Section~\ref{sec:two-point} above),
then $\partial \gamma_1 / \partial \theta > 0$
and $\partial \gamma_2 / \partial \theta < 0$.
\end{lemma}
\begin{proof}
Under the assumptions, we have $C_1 < \mu < C_2$ by Theorem~\ref{th:fuzzy-df}.
To simplify notation
assume $T(X) = X$ so we have a standard exponential family, assume
$j = 3 - i$, and define
\begin{align*}
   A_\text{in} & = \set{ x : C_1 < x < C_2 }
   \\
   A_\text{out} & = \set{ x : x < C_1 \opor C_2 < x }
\end{align*}
(either of these can be empty, but not both by the more than two points
assumption.

Now we can rewrite both \eqref{eq:gamma-1} and \eqref{eq:gamma-2} as
\begin{align*}
   \gamma_i
   & =
   \frac{\alpha (C_j - \mu) + (M_i - C_j P_i) + (M_j - C_j P_j)}
   {p_i (C_j - C_i)}
   \\
   & =
   \frac{E_\theta\{ (X - C_j) [ I_{A_\text{out}}(X) - \alpha ] \}}
   {f_\theta(C_i) (C_j - C_i)}
   \\
   & =
   \int
   \frac{(x - C_j) [ I_{A_\text{out}}(x) - \alpha ] }{C_j - C_i}
   \cdot
   \frac{f_\theta(x)}{f_\theta(C_i)} \lambda(x)
   \\
   & =
   \int
   \frac{(x - C_j) [ I_{A_\text{out}}(x) - \alpha ] }{C_j - C_i}
   \cdot e^{(x - C_i) \theta} \lambda(x)
\end{align*}
where $\theta$ is now the canonical parameter, $f_\theta$ is the probability
mass function, and $\lambda$ is a discrete positive measure that does not
depend on $\theta$ (Section~\ref{sec:fundamentals} above).

This equation can be differentiated under the integral sign giving
\begin{equation} \label{eq:derivative}
   \frac{\partial \gamma_i}{\partial \theta}
   =
   \int
   \frac{(x - C_i) (x - C_j) [ I_{A_\text{out}}(x) - \alpha ] }{C_j - C_i}
   \cdot e^{(x - C_i) \theta} \lambda(x)
\end{equation}
\citep[Lemma of Chapter~18]{ferguson}.  The dominating function in Ferguson's
lemma can be taken to be a constant plus
$e^{x (\theta - \varepsilon)} + e^{x (\theta + \varepsilon)}$ which is
guaranteed to have finite integral for some $\varepsilon > 0$ by the
assumption that we have a regular exponential family.

In case $i = 1$ and $0 < \alpha < 1$ in \eqref{eq:derivative}
the integrand is positive for $x \in A_\text{out} \cup A_\text{in}$ and
zero otherwise.  Hence the derivative is positive.

In case $i = 2$ and $0 < \alpha < 1$ in \eqref{eq:derivative}
the integrand is negative for $x \in A_\text{out} \cup A_\text{in}$ and
zero otherwise.  Hence the derivative is negative.
\end{proof}

\begin{lemma} \label{lem:gamma-derivative-ump}
In \eqref{eq:gamma-ump}, if $C$ is chosen
so that $\gamma$ is strictly
between zero and one (so this formula defines the critical function of a
UMP upper-tailed test), and if the support of the canonical statistic has
more than one point, then $\partial \gamma / \partial \theta < 0$.

For the UMP lower-tailed test all inequalities are reversed so,
$\partial \gamma / \partial \theta > 0$.
\end{lemma}
\begin{proof}
As in the preceding lemma, we rewrite \eqref{eq:gamma-ump}
\begin{align*}
   \gamma
   & =
   \frac{E_\theta\{ \alpha - I_{(C, \infty)}(X)\}}{f_\theta(C)}
   \\
   & =
   \int
   [ \alpha - I_{(C, \infty)}(x) ] \cdot \frac{f_\theta(x)}{f_\theta(C)}
   \\
   & =
   \int
   [ \alpha - I_{(C, \infty)}(x) ] e^{(x - C) \theta} \lambda(x)
\end{align*}
(with the same notation as in the preceding lemma).
This equation can be differentiated under the integral sign giving
\begin{equation} \label{eq:derivative-ump}
   \frac{\partial \gamma}{\partial \theta}
   =
   \int
   [ \alpha - I_{(C, \infty)}(x) ] (x - C) e^{(x - C) \theta} \lambda(x)
\end{equation}
(with argument about differentiation under the integral sign as in the
preceding lemma).
The integrand is negative when $x \neq C$ and
zero otherwise.  Hence the derivative is negative.
\end{proof}

\begin{theorem} \label{th:fuzzy-convex}
With the assumptions of Theorem~\ref{th:fuzzy-df},
a fuzzy confidence interval for the canonical parameter or any increasing
function of it (including the mean value parameter) corresponding to
a UMP or UMPU test for a regular full exponential family whose canonical
statistic is probabilistically and order discrete is convex,
and its membership function is continuous.

Let $\mu$ denote the mean value parameter and $x$ the observed
value of the canonical statistic.  The membership function is nondecreasing
for $\mu < x$ and nonincreasing for $\mu > x$.
If the maximum less than one, then the maximum occurs at $\mu = x$
where \eqref{eq:umpu-spec-a} and \eqref{eq:umpu-spec-b} and \eqref{eq:gamma-1}
and \eqref{eq:gamma-2} all hold simultaneously.
If the maximum of the membership function is one,
then $\mu = x$ is one point where it is one.
\end{theorem}
To say a fuzzy set is convex is to say
that each of the level sets $\set{x : I(x) \ge \gamma}$
of its membership function $I$ are convex
\citep[Section~1.3]{geyer-meeden}.

Note that this does not mean that the graph of the membership function
or any part of it is convex or concave.
See examples in Appendix~\ref{sec:fci-plots}.

\begin{proof}
If the membership function of the fuzzy confidence interval for the canonical
parameter $\theta$ is convex, then so is the same for any increasing function 
of $\theta$.  The mean value parameter is an increasing function of the
canonical parameter (Section~\ref{sec:fundamentals} above).

From the lemmas, the negative derivatives are for $\gamma_2$ for the UMPU
test or for $\gamma$ of the UMP upper-tailed test.  Thus these are decreasing
functions of $\theta$.  If they ever decrease to zero, then we can change
the representation of the critical function without changing its value,
by increasing $C_2$ or $C$
to the next possible value and setting $\gamma_2$ or $\gamma$ to one.

And similarly for $\gamma_1$ for the UMPU test or for $\gamma$ for the UMP
lower-tailed test.  These are increasing functions of $\theta$.
If they ever increase to one, then we can change
the representation of the critical function without changing its value,
by increasing $C_1$ or $C$
to the next possible value and setting $\gamma_1$ or $\gamma$ to zero.

Thus the critical function $\phi(x, \alpha, \theta)$ is continuous in
$\theta$, and $C_1$ and $C_2$ and $C$ can only increase as $\theta$ increases.
Hence the membership function of the fuzzy confidence interval corresponding
to the upper-tailed UMP test is nondecreasing in $\theta$,
for the lower-tailed UMP test it is nonincreasing,
and for the two-tailed UMPU test it may be constant
at zero for a while (when $C_2 < x$), then increase (when $C_2 = x$),
then be constant at one for a while (when $C_1 < x < C_2$),
then decrease (when $C_1 = x$), then be constant at zero for a while
(when $x < C_1$).  But some of these
parts may be omitted for certain $x$ (more on this below).  For any of these
the fuzzy confidence interval is convex.

The only way we can have $\phi(x, \alpha, \theta)$ strictly
between zero and one is when $x = C_1$ or $x = C_2$.  If the latter,
then $\mu \le C_2 = x$, and the membership function is decreasing by
Lemma~\ref{lem:gamma-derivative}.  If the former,
then $x = C_1 \le \mu$, and the membership function is increasing by
the same lemma.  And, of course, the membership function is constant
on intervals where it is equal to zero or one.  This proves the
last paragraph of the theorem statement.

So the proof is finished except that we still must look at the case where
the mean value parameter $\mu$ crosses a possible value of
the canonical statistic.
There we may have \eqref{eq:umpu-spec-a} and \eqref{eq:umpu-spec-b} holding
when $\mu = C$, but have \eqref{eq:gamma-1} and \eqref{eq:gamma-2} on either
side.  So we need to check that we have continuity here.
Rather than \eqref{eq:gamma-1} and \eqref{eq:gamma-2}, it is more convenient
to use \eqref{eq:gamma-1-alt} and \eqref{eq:gamma-2-alt}, which in this case
(where $C_1$ and $C_2$ are adjacent and $\mu$ is between) become
\begin{align*}
   1 - \gamma_1 & = \frac{(1 - \alpha) (C_2 - \mu)}{f_\mu(C_1) (C_2 - C_1)}
   \\
   1 - \gamma_2 & = \frac{(1 - \alpha) (\mu - C_1)}{f_\mu(C_2) (C_2 - C_1)}
\end{align*}
And these give us $1 - \gamma_1 \to (1 - \alpha) / f_\mu(C_1)$ agreeing
with \eqref{eq:umpu-spec-a} and $\gamma_2 \to 0$ as $\mu \to C_1$.
The argument for $\mu \to C_2$ is similar.
Thus we do have continuity when we switch from
\eqref{eq:umpu-spec-a} and \eqref{eq:umpu-spec-b}
to \eqref{eq:gamma-1} and \eqref{eq:gamma-2}.
\end{proof}

\subsubsection{Endpoint Behavior}
\label{sec:endpoint}

The UMPU test makes no sense when the null hypothesis is on
the boundary of the mean value parameter space.
Why do a two-tailed test when the null hypothesis says only one tail
is possible?

But equations \eqref{eq:umpu-crit}, \eqref{eq:umpu-a},
and \eqref{eq:umpu-b} still make sense and define a test,
which is the limit of tests done for all nearby parameter
values hypothesized under the null hypothesis.  Since
the probability and the expectation in those equations are continuous
in $\theta$ this also characterizes the behavior as $\theta$ converges
to a boundary point (which we need to know to calculate fuzzy confidence
intervals, which involve all $\theta$ in the parameter space).

\begin{theorem} \label{th:umpu-end}
With the assumptions of Theorem~\ref{th:fuzzy-df},
if the support of $T(X)$ has a lower bound and $L$ is the two-point
set consisting of the two lowest atoms of the support, then
\begin{equation} \label{eq:umpu-end}
   1 - \phi(x, \alpha, \theta) \to (1 - \alpha) I_L\{T(x)\},
   \qquad \text{as $\theta \to - \infty$},
\end{equation}
where $\theta$ is the canonical parameter.
Similarly, if the support has an upper bound and $L$ consists of the two
highest atoms, then \eqref{eq:umpu-end} holds with $- \infty$ replaced by
$+ \infty$.
\end{theorem}
\begin{proof}
We do the case where the support has a lower bound.  The upper bound
case is entirely analogous.

Without loss of generality, we assume $T(X) = X$, and use most of
Section~\ref{sec:fundamentals} above.
Because $X$ is bounded below (we assume), the full canonical parameter
space \eqref{eq:full-canonical-parameter-space} extends to $- \infty$.
Because we assume the support of the distribution has more than one point
\eqref{eq:cumfun-second-deriv} cannot be zero, and
$\mu$ and $\theta$ are strictly increasing continuous functions of each other.

Let $L = \{ s_0, s_1 \}$ with $s_0 < s_1$ be the set containing the two
lowest points of the support of $X$.  Let $S$ denote the support.
Since
$$
   \frac{f_\theta(s)}{f_\theta(s_0)}
   = e^{(s - s_0) \theta} \frac{\lambda(s)}{\lambda(s_0)}
$$
the distribution converges to the distribution concentrated at
$s_0$ as $\theta \to - \infty$ by monotone convergence.

Now
$$
   \frac{\mu(\theta) - s_0}{f_\theta(s_1)}
   =
   \sum_{s \in S \setminus \{s_0\}} (s - s_0) e^{(s - s_1) \theta}
   \frac{\lambda(s)}{\lambda(s_1)}
$$
goes to $s_1 - s_0$ by monotone convergence as $\theta \to - \infty$.
Hence $\mu(\theta) \to s_0$ as $\theta \to - \infty$.

Now
$$
   \frac{\pr_\theta\{T(X) > s_1\}}{f_\theta(s_1)}
   =
   \sum_{s \in S \setminus L}
   e^{(s - s_1) \theta} \frac{\lambda(s)}{\lambda(s_1)}
$$
goes to zero by monotone convergence as $\theta \to - \infty$.

And these facts together imply
\begin{equation} \label{eq:expo-gen}
\begin{split}
   \pr_\mu\{T(X) = s_0\} & = \frac{s_1 - \mu}{s_1 - s_0} + o(\mu - s_0)
   \\
   \pr_\mu\{T(X) = s_1\} & = \frac{\mu - s_0}{s_1 - s_0} + o(\mu - s_0)
   \\
   \pr_\mu\{T(X) > s_1\} & = o(\mu - s_0)
\end{split}
\end{equation}
where $\mu = \mu(\theta)$ is the mean value parameter.

Now we claim that for low enough values of $\theta$
the UMPU test is given by \eqref{eq:umpu-crit} with
$C_1 = s_0$ and $C_2 = s_1$ and $\gamma_1$ and $\gamma_2$ given by
\eqref{eq:gamma-1-alt} and \eqref{eq:gamma-2-alt}, which in this case become
\begin{align*}
   \gamma_1
   & =
   1 - \frac{(1 - \alpha) (s_1 - \mu)}{\pr_\mu\{T(X) = s_0\} (s_1 - s_0)}
   \\
   \gamma_2
   & =
   1 - \frac{(1 - \alpha) (\mu - s_0)}{\pr_\mu\{T(X) = s_1\} (s_1 - s_0)}
\end{align*}
and clearly both converge to $\alpha$ as $\mu \to s_0$ hence both are
between zero and one for low enough $\theta$ or $\mu$ and hence
define the UMPU test.
\end{proof}

This explains the behavior of the
fuzzy confidence intervals for the binomial distribution for
the two $x$ values nearest each boundary in Figure~2 of \citet{geyer-meeden}.
As $\theta \to 0$, the fuzzy confidence interval $1 - \phi(x, \alpha, \theta)$
converges to $1 - \alpha$ for $x = 0$ or $x = 1$ and converges to zero
for all other $x$.
And as $\theta \to 1$, the fuzzy confidence interval
converges to $1 - \alpha$ for $x = n - 1$ or $x = n$ and converges to zero
for all other $x$.

\begin{theorem} \label{th:umpu-end-unbounded}
With the assumptions of Theorem~\ref{th:fuzzy-df},
if the support of $T(X)$ has no upper bound, then
$C_1$ and $C_2$ in \eqref{eq:umpu-crit} both go to infinity as the mean value parameter
goes to infinity.
\end{theorem}
\begin{proof}
Since $C_1 \le \mu \le C_2$, only the behavior of $C_1$ is at issue.
And $C_1$ is a nondecreasing function of $\theta$ by the proof of
Theorem~\ref{th:fuzzy-convex}.
Hence either $C_1 \to \infty$ as $\mu \to \infty$ or there is a finite $b$
such that $C_1 \le b$ for all $\mu$.  We start a proof by contradiction
by assuming the latter.

The full canonical parameter space \eqref{eq:full-canonical-parameter-space}
is an open interval (the regular full
exponential family assumption).  Let $B$ denote its upper bound, which
may be finite or infinite.
We have two different arguments depending on whether $B$ is finite or not.

First assume $B$ is infinite, if $x_1 < x_2$, then
$$
   \frac{f_\theta(x_1)}{f_\theta(x_2)} = e^{(x_1 - x_2) \theta}.
$$
As $\theta \to \infty$ the right-hand side goes to zero.
But (for discrete data) probabilities are bounded by one so
\begin{equation} \label{eq:limit-zero}
   f_\theta(x) \to 0, \qquad \text{as $\theta \to B$ and $\mu \to \infty$}.
\end{equation}

Second assume $B$ is finite, then
$$
   f_\theta(x) \le e^{x B - c(\theta)}
$$
but $c(\theta) \to \infty$ as $\theta \to B$
(end of Section~\ref{sec:fundamentals} above).
So again we have \eqref{eq:limit-zero}.

Now let $A$ be any event that is bounded above (in the mean value parameter
space, where $X$ also takes values).  Then for any nonnegative function $g$
$$
    E_\mu \{ I_A(X) g(X) \} \to 0, \qquad \text{as $\mu \to \infty$}
$$
by monotone convergence (because the integrand is decreasing after $\mu$
is greater than any element of $A$), provided the expectation exists for
some $\mu$ greater than any element of $A$.

It follows that as $\mu \to \infty$
\begin{align*}
   E_\mu\{I_{(- \infty, \mu)}(X) \phi(X, \alpha, \mu)\} & \to 0
   \\
   E_\mu\{X I_{(- \infty, \mu)}(X) \phi(X, \alpha, \mu)\} & \to 0
   \\
   E_\mu\{I_{(\mu, \infty)}(X) \phi(X, \alpha, \mu)\} & \to \alpha
   \\
   E_\mu\{X I_{(\mu, \infty)}(X) \phi(X, \alpha, \mu)\} & \to \alpha \mu
\end{align*}
but the latter two are impossible because
$$
   E_\mu\{X I_{(\mu, \infty)}(X) \phi(X, \alpha, \mu)\}
   \ge
   C_2 \cdot E_\mu\{I_{(\mu, \infty)}(X) \phi(X, \alpha, \mu)\}
$$
and the latter goes to infinity.  Hence the assumption to get a contradiction
is false, and $C_1 \to \infty$.
\end{proof}

\subsubsection{Summary} \label{sec:summary-confint}

Fuzzy confidence intervals (more precisely, the membership function
thereof, but we won't be pedantic about that here)  behave as follows.
\begin{itemize}
\item If the sample space of the canonical statistic has two points,
then the fuzzy confidence interval is constant, equal to $1 - \alpha$ for all
values of the parameter.  Henceforth we assume the sample space has
more than two points.
\item If the observed value $x$ of the canonical statistic is as low as
possible, then the fuzzy confidence interval starts at $1 - \alpha$
(when $\mu$ is as low as possible) and decreases to zero (at some point
where $\mu$ is not as high as possible).  And is zero thereafter
(by convexity).
\item If the observed value $x$ of the canonical statistic is the next
to lowest
possible, then the fuzzy confidence interval starts at $1 - \alpha$
(when $\mu$ is as small as possible) and increases.  Depending on the
value of $\alpha$ it may go all the way up to one, or may not.
If it is less than one for all values of the parameter, then it reaches
its maximum when $x = \mu$ and equations \eqref{eq:umpu-spec-a}
and \eqref{eq:umpu-spec-b} hold.  Unless $x$ is one of the two highest
points of the support, the fuzzy confidence interval decreases to zero
and is zero thereafter.
\item If the observed value $x$ of the canonical statistic is not either
of the two lowest possible values or either of the two highest possible
values, then the fuzzy confidence interval starts at zero
(when $\mu$ is as small as possible), stays at zero for a while, and
then increases (as $\mu$ increases).  Depending on the
value of $\alpha$ it may go all the way up to one, or may not.
If it is less than one for all values of the parameter, then it reaches
its maximum when $x = \mu$ and equations \eqref{eq:umpu-spec-a}
and \eqref{eq:umpu-spec-b} hold.  After reaching the maximum, it decreases
to zero, and is zero thereafter.
\end{itemize}
Appendix~\ref{sec:fci-plots} has concrete examples of this behavior.

\subsection{Models With Nuisance Parameters}

UMP and UMPU theory extends to multiparameter exponential families
when the parameter of interest $\theta$ is one of the canonical parameters
(Lehmann, TSH, 1st ed.,\ pp.\ 134--136).

Suppose the family has densities of the form
$$
   \frac{1}{c(\theta, \boldsymbol{\eta})}
   \exp\left( \theta T(x) + \sum_{i = 1}^k \eta_i U_i(x) \right)
$$
with respect to some measure on the sample space.  Then the situation
is exactly the same as described above except that the reference distribution
of the test is the conditional distribution of $T(X)$ given $\mathbf{U}(X)$,
which (a standard fact about exponential families) depends only on $\theta$
and not on the nuisance parameter $\boldsymbol{\eta}$.

\subsubsection{UMP Tests With Nuisance Parameters}

Now there exists a UMP test
having null hypothesis $H_0 = \set{ \vartheta : \vartheta \le \theta }$,
alternative hypothesis $H_1 = \set{ \vartheta : \vartheta > \theta }$,
and significance level $\alpha$,
and its critical function $\phi$ is defined by
\begin{equation} \label{eq:crit-lower}
\begin{split}
   \phi(x, \alpha, \theta)
   =
   \begin{cases}
   1, & T(x) > C[\mathbf{U}(x)]
   \\
   \gamma[\mathbf{U}(x)], & T(x) = C[\mathbf{U}(x)]
   \\
   0, & T(x) < C[\mathbf{U}(x)]
   \end{cases}
\end{split}
\end{equation}
where the functions $\gamma$ and $C$ are determined by
\begin{equation} \label{eq:cond-lower}
   E_\theta\{\phi(X, \alpha, \theta) \mid \mathbf{U}(X) \} = \alpha.
\end{equation}

Everything is exactly the same as for the one-parameter case except
for the conditioning on $\mathbf{U}(x)$.  The only point of the discussion
is that the test is UMP whether considered conditionally or unconditionally.

As before, the UMP upper-tailed test is obtained by reversing all the
inequalities above.

\subsubsection{UMPU Tests With Nuisance Parameters}

Now there exists a UMPU test
having null hypothesis $H_0 = \set{ \vartheta : \vartheta = \theta }$,
alternative hypothesis $H_1 = \set{ \vartheta : \vartheta \neq \theta }$,
and significance level $\alpha$,
and its critical function $\phi$ is defined by
\begin{equation} \label{eq:crit-two}
\begin{split}
   \phi(x, \alpha, \theta)
   =
   \begin{cases}
   1, & T(x) < C_1[\mathbf{U}(x)]
   \\
   \gamma_1[\mathbf{U}(x)], & T(x) = C_1[\mathbf{U}(x)]
   \\
   0, & C_1[\mathbf{U}(x)] < T(x) < C_2[\mathbf{U}(x)]
   \\
   \gamma_2[\mathbf{U}(x)], & T(x) = C_2[\mathbf{U}(x)]
   \\
   1, & C_2[\mathbf{U}(x)] < T(x)
   \end{cases}
\end{split}
\end{equation}
where the functions $\gamma_1$, $\gamma_2$, $C_1$, and $C_2$ are determined by
\begin{subequations}
\begin{align}
   E_\theta\{\phi(X, \alpha, \theta) \mid \mathbf{U}(X) \} & = \alpha
   \label{eq:cond-two-a}
   \\
   E_\theta\{T(X) \phi(X, \alpha, \theta) \mid \mathbf{U}(X) \} & = \alpha
   E_\theta\{T(X) \mid \mathbf{U}(X) \}
   \label{eq:cond-two-b}
\end{align}
\end{subequations}

Again, the point
is that the test is UMPU whether considered conditionally or unconditionally.

% As we saw in Section~\ref{sec:special}, it is possible that $C_1 = C_2$
% in which case $\gamma_1$ and $\gamma_2$ are not separately determinable,
% only their sum.  Analysis of that situation here would just repeat that
% section word for word except for references to conditioning on $\mathbf{U}(x)$.

\section{Applications} \label{sec:models}

\subsection{Binomial} \label{sec:model-binomial}

Let $X \sim \BinomialDis(n, p)$ with $0 < p < 1$.

All of the quantities in \eqref{eq:gamma-1} and \eqref{eq:gamma-2}
are easily calculated (in R) except possibly $M_1$ and $M_2$.  Actually,
as Lehmann points out (TSH, 1st, ed.,\ pp.~128--129), these are
also easy to calculate
\begin{align*}
   M_1
   & =
   \sum_{x = 0}^{C_1 - 1} x \binom{n}{x} p^x (1 - p)^{n - x}
   \\
   & =
   \sum_{x = 1}^{C_1 - 1} x \binom{n}{x} p^x (1 - p)^{n - x}
   \\
   & =
   n p \sum_{x = 1}^{C_1 - 1} \binom{n - 1}{x - 1} p^{x - 1} (1 - p)^{n - x}
   \\
   & =
   n p \sum_{y = 0}^{C_1 - 2} \binom{n - 1}{y} p^y (1 - p)^{n - 1 - y}
\end{align*}
and the last sum is just a binomial probability for the
$\BinomialDis(n - 1, p)$ distribution, that is, $M_1$ is calculated
in R (with the obvious definitions of the variables) by
\begin{verbatim}
n * p * pbinom(c1 - 2, n - 1, p)
\end{verbatim}
Let's check that this does not crash for small values of $C_1$.
<<<check-c1>>=
n <- 10
p <- 2 / 3
c1 <- seq(0, n * p)
c1
m1 <- n * p * pbinom(c1 - 2, n - 1, p)
m1
@

And compare with obvious calculation
<<check-c1-too>>=
m1.too <- cumsum((c1 - 1) * dbinom(c1 - 1, n, p))
all.equal(m1, m1.too)
@

Similarly, exchanging successes and failures,
$$
   M_2
   =
   n p \sum_{y = C_2}^{n - 1} \binom{n - 1}{y - 1} p^y (1 - p)^{n - 1 - y}
$$
So $M_2$ is calculated by
\begin{verbatim}
n * p * pbinom(c2 - 1, n - 1, p, lower.tail = FALSE)
\end{verbatim}
Try that out too
<<<check-c2>>=
n <- 10
p <- 2 / 3
c2 <- seq(n, n * p) |> rev()
c2
m2 <- n * p * pbinom(c2 - 1, n - 1, p, lower.tail = FALSE)
m2
@

And compare with obvious calculation
<<check-c2-too>>=
m2.too <- rev(cumsum(rev((c2 + 1) * dbinom(c2 + 1, n, p))))
all.equal(m2, m2.too)
@

\subsection{Poisson} \label{sec:model-poisson}

Let $X \sim \PoissonDis(\mu)$ with $0 < \mu$.

Again, all of the quantities in \eqref{eq:gamma-1} and \eqref{eq:gamma-2}
are easily calculated except possibly $M_1$ and $M_2$.  Do they work
like the binomial case?  Yes!
\begin{align*}
   M_1
   & =
   \sum_{x = 0}^{C_1 - 1} x \frac{\mu^x}{x !} e^{- \mu}
   \\
   & =
   \sum_{x = 1}^{C_1 - 1} x \frac{\mu^x}{x !} e^{- \mu}
   \\
   & =
   \mu \sum_{x = 1}^{C_1 - 1} \frac{\mu^{x - 1}}{(x - 1) !} e^{- \mu}
   \\
   & =
   \mu \sum_{y = 0}^{C_1 - 2} \frac{\mu^y}{y !} e^{- \mu}
\end{align*}
and the last sum is just another Poisson probability,
that is, $M_1$ can be calculated
in R (with the obvious definitions of the variables) by
\begin{verbatim}
mu * ppois(c1 - 2, mu)
\end{verbatim}
and $M_2$ by
\begin{verbatim}
mu * ppois(c2 - 1, mu, lower.tail = FALSE)
\end{verbatim}

\subsection{Negative Binomial} \label{sec:negbin}

Let $X \sim \NegativeBinomialDis(r, p)$ with $0 < p < 1$.  Like R we consider
the sample space to start at zero rather than $r$.  This also allows for
non-integer $r$.  The densities of the family have the form
$$
   f(x) = \frac{\Gamma(x+r)}{\Gamma(r) x!} p^r (1-p)^x
$$

Note that if we are to have an exponential family $r$ cannot be an unknown
parameter!  The only unknown parameter is $p$.

Again, all of the quantities in \eqref{eq:gamma-1} and \eqref{eq:gamma-2}
are easily calculated except possibly $M_1$ and $M_2$.  Do these
work like the binomial and Poisson cases?  Yes!
\begin{align*}
   M_1
   & =
   \sum_{x = 0}^{C_1 - 1} x \frac{\Gamma(x+r)}{\Gamma(r) x!} p^r (1-p)^x
   \\
   & =
   \sum_{x = 1}^{C_1 - 1} x \frac{\Gamma(x+r)}{\Gamma(r) x!} p^r (1-p)^x
   \\
   & =
   \sum_{y = 0}^{C_1 - 2} \frac{\Gamma(y+1+r)}{\Gamma(r) y!} p^r (1-p)^{y + 1}
   \\
   & =
   \frac{1 - p}{p}
   \sum_{y = 0}^{C_1 - 2} \frac{\Gamma(y+1+r)}{\Gamma(r) y!} p^{r + 1} (1-p)^y
\end{align*}
$M_1$ can be calculated
in R (with the obvious definitions of the variables) by
\begin{verbatim}
(1 - p) / p * pnbinom(c2 - 2, r + 1, p)
\end{verbatim}
and $M_2$ can be calculated by
\begin{verbatim}
(1 - p) / p * pnbinom(c1 - 1, r + 1, p, lower.tail = FALSE)
\end{verbatim}

\subsection{Two Independent Poisson Random Variables}
\label{sec:two-poisson-indep}

Let $X_i \sim \PoissonDis(\mu_i)$ with $0 < \mu_i$, for $i = 1$, $2$
be independent random variables.  We wish to compare the means $\mu_1$ and
$\mu_2$.  We cannot just test or produce fuzzy confidence intervals for
a function pulled out of the air, such as $\mu_1 - \mu_2$.  The parameter
we test must be canonical.

The canonical statistics of this exponential family are $X_1$ and $X_2$
and the corresponding canonical parameters are $\psi_i = \log(\mu_i)$.
Linear functions of canonical parameters are again canonical so we can test
or produce fuzzy confidence intervals
for $\psi_1 - \psi_2 = \log(\mu_1 / \mu_2)$.

Introduce new parameters
\begin{align*}
   \psi_1 & = \eta + \theta
   \\
   \psi_2 & = \eta
\end{align*}
Then
$$
   X_1 \psi_1 + X_2 \psi_2
   =
   X_1 \theta + (X_1 + X_2) \eta
   =
   T(\mathbf{X}) \theta + U(\mathbf{X}) \eta
$$
Where
\begin{equation} \label{eq:t-and-u}
\begin{split}
   T(\mathbf{X}) & = X_1
   \\
   U(\mathbf{X}) & = X_1 + X_2
\end{split}
\end{equation}

It is a standard result that the conditional distribution of
$T(\mathbf{X})$ given $U(\mathbf{X})$ is
$$
   X_1 \mid X_1 + X_2
   \sim
   \BinomialDis\left(X_1 + X_2, \frac{\mu_1}{\mu_1 + \mu_2}\right)
$$
So the theory says we do the UMP or UMPU test based on this distribution
with $\mu_1 / (\mu_1 + \mu_2)$ as the parameter of interest
(Lehmann, TSH, 1st ed., pp.~140--142, gives further details).

\subsection{Two Independent Binomial Random Variables}
\label{sec:two-binom-indep}

Let $X_i \sim \BinomialDis(n_i, p_i)$ with $0 < p_i < 1$, for $i = 1$, $2$
be independent random variables.  We wish to compare the proportions $p_1$ and
$p_2$.  We cannot just test or produce fuzzy confidence intervals for
a function pulled out of the air, such as $p_1 - p_2$.  The parameter
we test must be canonical.

The canonical statistics of this exponential family are $X_1$ and $X_2$
and the corresponding canonical parameters are $\psi_i = \logit(p_i)$.
Linear functions of canonical parameters are again canonical so we can test
or produce fuzzy confidence intervals for $\psi_1 - \psi_2$.

As in the Poisson case we see that we can base the test on the conditional
distribution of $T(\mathbf{X})$ given $U(\mathbf{X})$, where these variables
are defined by \eqref{eq:t-and-u}.
This distribution is (Lehmann, TSH, 1st ed., pp.~142--143) the exponential
family generated by the hypergeometric distribution, which is called
Fisher's noncentral hypergeometric distribution
(Section~\ref{sec:noncentral-hypergeometric} above).
It has canonical parameter
$$
   \theta = \log\left( \frac{p_1 (1 - p_2)}{(1 - p_1) p_2} \right)
$$
So the theory says we do the UMP or UMPU test based on this distribution
with $\theta$ as the parameter of interest

\subsection{Two Independent Negative Binomial Variables}
\label{sec:two-negbin-indep}

Let $X_i \sim \NegativeBinomialDis(r_i, p_i)$ with $0 < r_i$ and
$0 < p_i < 1$, for $i = 1$, $2$ be independent random variables.
As in Section~\ref{sec:negbin} we are using the convention that the
sample space starts at zero.  We wish to compare the proportions $p_1$ and
$p_2$.  We cannot just test or produce fuzzy confidence intervals for
a function pulled out of the air, such as $p_1 - p_2$.  The parameter
we test must be canonical.

The canonical statistics of this exponential family are $X_1$ and $X_2$
and the corresponding canonical parameters are $\psi_i = \log(1 - p_i)$.
Linear functions of canonical parameters are again canonical so we can test
or produce fuzzy confidence intervals for $\psi_1 - \psi_2$.

As in the Poisson case we see that we can base the test on the conditional
distribution of $T(\mathbf{X})$ given $U(\mathbf{X})$, where these variables
are defined by \eqref{eq:t-and-u}.
This distribution is not fully explained in Lehmann, although
the $r_1 = r_2 = 1$ case is the subject of a homework problem in the
second edition.

Let's see what happens.  The joint distribution of the $X$'s is
\begin{align}
   f(x_1, x_2)
   & =
   \prod_{i = 1}^2
   \frac{\Gamma(x_i + r_i)}{\Gamma(r_i) x_i!} p_i^{r_i} (1 - p_i)^{x_i}
   \nonumber
   \\
   & =
   \exp(x_1 \psi_1 + x_2 \psi_2)
   \prod_{i = 1}^2
   \frac{\Gamma(x_i + r_i)}{\Gamma(r_i) x_i!} p_i^{r_i}
   \nonumber
   \\
   & =
   \exp(t \theta + u \eta)
   \frac{\Gamma(t + r_1)}{\Gamma(r_1) t!}
   \frac{\Gamma(u - t + r_2)}{\Gamma(r_2) (u - t)!}
   p_1^{r_1} p_2^{r_2}
   \label{eq:unnormal}
\end{align}
where
\begin{align*}
   t & = x_1
   \\
   u & = x_1 + x_2
   \\
   \theta & = \psi_1 - \psi_2 = \log\left(\frac{1 - p_1}{1 - p_2}\right)
   \\
   \eta & = \psi_2 = \log(1 - p_2)
\end{align*}
We want to consider the conditional distribution of $T(\mathbf{X})$
given $U(\mathbf{X})$.
Thought of as a function of $t$ for fixed $u$ and dropping all terms
that do not contain $t$ and $\theta$ we get
\begin{equation} \label{eq:neg-hyper-expo}
   f_\theta(t \mid u)
   =
   \frac{1}{c(\theta)}
   \exp(t \theta)
   \frac{\Gamma(t + r_1) \Gamma(u - t + r_2)}{t! (u - t)!},
   \qquad t = 0, \ldots, u,
\end{equation}
where
$c(\theta)$ is chosen to make probabilities sum to one.

Equations \eqref{eq:neg-hyper-expo} and
\eqref{eq:noncentral-negative-hypergeometric-pmf} agree except
the following changes of variable names
\begin{center}
\begin{tabular}{cc}
\eqref{eq:neg-hyper-expo} & \eqref{eq:noncentral-negative-hypergeometric-pmf}
\\
\hline
$t$ & $x$ \\
$r_1$ & $r$ \\
$u + r_2$ & $N - r$ \\
$u$ & $K$
\end{tabular}
\end{center}
Thus we have indeed arrived at the exponential family described in
Section~\ref{sec:noncentral-negative-hypergeometric} above.

\subsection{Testing Independence in a Two-by-two Table} \label{sec:exact}

This is the UMP/UMPU competitor for Fisher's exact test.  The data consist
of a matrix $X_{i j}$, $i = 1$, 2, $j = 1$, 2, that has a multinomial
distribution with sample size $n$ and cell probability matrix $p_{i j}$,
$i = 1$, 2, $j = 1$, 2.  This is also called a two-by-two contingency table.

The canonical statistics are the $X_{i j}$, but the canonical parameters are
not uniquely defined in terms of the $p_{i j}$ because the model is really
only three dimensional, not four, because the $X_{i j}$ sum to $n$.

As is well known, this is a three-dimensional exponential family, the
canonical statistics being any three of the four $X_{i j}$, the fourth
being determined from the other three by the requirement that the $X_{i j}$
sum to $n$.

In this problem (Lehmann, TSH, 1st ed., pp.~143--146) the marginals
are the statistics for the nuisance parameters,
and we can consider any other statistic
linearly independent of the marginals and the sum of all cells as the
statistic of interest.  Lehmann chooses
\begin{equation} \label{eq:exact}
\begin{split}
   T(\mathbf{X}) & = X_{1 1}
   \\
   U_1(\mathbf{X}) & = X_{1 1} + X_{1 2}
   \\
   U_2(\mathbf{X}) & = X_{1 1} + X_{2 1}
\end{split}
\end{equation}

The conditional distribution of $T(\mathbf{X})$ given the marginals
$U_1(\mathbf{X})$ and $U_2(\mathbf{X})$ is well known.  It is the
hypergeometric distribution involved in Fisher's exact test under the
null hypothesis of independence and under general null hypotheses is
the exponential family generated by the hypergeometric we encountered
in Sections~\ref{sec:noncentral-hypergeometric} and~\ref{sec:two-binom-indep}.
The canonical parameter is
$$
   \theta = \log \left( \frac{p_{1 1} p_{2 2}}{p_{1 2} p_{2 1}} \right)
$$

\subsection{Different Answers to the Same Question in a Poll}
\label{sec:different-answers-same-question}

This section and the next give the UMP/UMPU/fuzzy competitors to the
analysis of correlated binomial proportions in Wild and Seber
(pp.~343--350).  The first considers different answers to the same
question on a poll.  This is a multinomial problem.  Say the categories
of interest have counts $X_1$ and $X_2$, then we know
$$
   X_1 \mid X_1 + X_2
   \sim
   \BinomialDis\left(X_1 + X_2, \frac{p_1}{p_1 + p_2}\right)
$$
and so the UMP/UMPU/fuzzy procedures are based on this distribution.

\subsection{Answers to Different Questions in a Poll}
\label{sec:answers-different-questions}

Here again, like in Section~\ref{sec:exact}, we have a two-by-two table
with data $X_{i j}$ and cell probabilities $p_{i j}$ but the question of
interest is different.  Now we are interested in just the opposite question,
whether the marginals differ.  This in a sense (a rather vague sense)
interchanges the role of interest and nuisance parameters, what were interest
parameters in Section~\ref{sec:exact} are now nuisance parameters and vice
versa.

Ordinarily, this would be nonsense.  There is exactly one interest parameter.
The rest (in this case two) must be nuisance parameters.  So,
strictly speaking, they cannot be interchanged.  But a two-by-two table has
a redundant canonical statistic: there are four $X_{i j}$ but they sum to $n$
so only three are linearly independent.  So if we add the redundant statistic
to the statistics corresponding to parameters of interest we two sets of two
that can be interchanged.

It is clear that \eqref{eq:exact} could have been written with subscripts
1 and 2 interchanged, which would make $X_{2 1}$ the statistic of interest.
This tells us that here we should condition on $X_{1 1}$ and $X_{2 2}$
leaving either $X_{1 2}$ or $X_{2 1}$ as the statistic of interest.
Thus in this case the UMP/UMPU/fuzzy procedure is based on the distribution
$$
   X_{1 2} \mid X_{1 1}, X_{2 2}
   \sim
   \BinomialDis\left(n - X_{1 1} - X_{2 2}, \frac{p_{1 2}}{p_{1 2} + p_{2 1}}
   \right)
$$

And in hindsight we see that we have invented the conditional, exact,
uniformly most whatever (UMW) competitor of McNemar's test (Lindgren,
\emph{Statistical Theory}, pp.~381--383).

\section{Algorithms} \label{sec:algorithms}

\subsection{Jobs}

The obvious solution to our problems is to provide a function
that calculates $\phi(x, \alpha, \theta)$ for each family of interest
and for UMP and UMPU.

Probably such a function should follow the usual recycling rule, documented,
for example, on the help page for \code{dbinom}, \code{pbinom}, \code{qbinom},
and \code{rbinom}
\begin{quotation}
The length of the result is determined by \code{n} for \code{rbinom}, and is
the maximum of the lengths of the numerical arguments for the other functions.

The numerical arguments other than \code{n} are recycled to the length
of the result.  Only the first elements of the logical arguments are used.
\end{quotation}

But rather than following this rule and only this rule, we should also
return values at the knots (which users cannot specify because they don't
know where they are).
\begin{itemize}
\item For fuzzy $P$-values, which are piecewise linear, there should at
    at least be an option to return only the knots and values at the knots,
    because that is all R needs to plot the function.
\item Since probability density functions are more easily interpreted than
    cumulative distribution functions, there should also be an option
    to return the derivative of the fuzzy $P$-value, which is undefined
    at the knots and constant between knots.  Of course, the derivative
    between two knots is given by rise over run (the difference of the
    values at those two knots divided by the difference of the knots,
    but that may involve catastrophic cancellation, and the computer
    should be more accurate).
\item Fuzzy confidence intervals are constant on the interval where they
    are equal to one (the core) so we can save memory both for the returned
    R object, and any plots made from it, by not returning any values in
    the core (even if asked for by the user), at least as an option.
    Instead we return the endpoints of the core, and the values (one)
    at these points.

    The set where a fuzzy confidence interval is nonzero is called its
    \emph{support} (not to be confused with the concept of support in
    probability theory (where the probability mass function is nonzero).
    We can also save memory by returning the endpoints of the support,
    the values there (perhaps zero) and the boundaries of the parameter space
    and the values there (perhaps zero) and points and values for no other
    points outside the support.
    
    Finally on the intervals where the fuzzy confidence intervals (where
    the membership function is nonlinear) we need to return the membership
    function values on a grid of parameter values and we need users to specify
    the density of points in the grid rather than the number of points because
    the users do not know the lengths of these intervals of rise and fall.
    Moreover, we need to insert the parameter values and membership function
    values at knots because users don't know those either, and not including
    the knots will make plots not show the knots.
\end{itemize}

\REVISED

But we are also interested in fuzzy $P$-values, where we compute
$\phi(x, \alpha, \theta)$ for fixed $x$ and $\theta$ but for all $\alpha$
and this is a continuous piecewise linear function, so rather than just
returning the values for a vector of $\alpha$ values, we could also return
the values at the knots.  Of course, it would be a different R function
that did that.

And we are also interested in fuzzy confidence intervals, where we compute
$1 - \phi(x, \alpha, \theta)$ for fixed $x$ and $\alpha$ but for all $\theta$
and this is a continuous function that is equal to one on an interval
(the \emph{core} of the fuzzy interval), equal to zero outside another
interval (the \emph{support} of the fuzzy interval), and nonlinear elsewhere.
So we want to distinguish the core and support, but otherwise just evaluate
at a vector of $\theta$ values.  Of course, it would be a different R function
that did that.

There is little interest in functions that vectorize over \code{x}.
Usually \code{x} is the observed data.  But there is no telling what
any user at any time in the future may want.  So this should be allowed.

For completeness, we should also provide fuzzy decision functions that
$\phi(x, \alpha, \theta)$ for fixed $\alpha$ and $\theta$ but for all $x$.
This function can return $C_1$ and $C_2$ and $\phi(x, \alpha, \theta)$
for $T(x) = C_1$ and $T(x) = C_2$, it being understood
that \eqref{eq:umpu-crit} gives the values for other $x$.

Thus we need
\begin{itemize}
\item a general function that calculates $\phi(x, \alpha, \theta)$ for
    any vectors $x$, $\alpha$, and $\theta$ following the usual recycling
    rule for arguments of different length.
\item a fuzzy $P$-value function that calculates $\phi(x, \fatdot, \theta)$ for
    scalar $x$ and $\theta$.  It returns vectors of knots and values of
    a continuous piecewise linear function.
\item a fuzzy confidence interval function that calculates
    $\phi(x, \alpha, \fatdot)$ for scalar $x$ and $\alpha$.  It returns
\begin{itemize}
\item the core (an interval),
\item the support (an interval containing the support),
\item a sequence of $\theta$ values between the lower endpoint of the support
    and the lower endpoint of the core and the corresponding
    $1 - \phi(x, \alpha, \theta)$ values,
\item a sequence of $\theta$ values between the upper endpoint of the core
    and the upper endpoint of the support and the corresponding
    $1 - \phi(x, \alpha, \theta)$ values.
\end{itemize}
\item a fuzzy decision function that calculates
    $\phi(\fatdot, \alpha, \theta)$ for scalar $\alpha$ and $\theta$.
    It returns $C_1$, $C_2$, $\gamma_1$, and $\gamma_2$ in
   \eqref{eq:umpu-crit}.
\end{itemize}

In the latter three cases (fuzzy $P$-value, fuzzy confidence interval,
fuzzy decision)
the function can gain considerable efficiency in doing its specialized job.

The package should also supply the usual d, p, q, and r functions
(like \code{dbinom}, \code{pbinom}, \code{qbinom}, and \code{rbinom})
for the noncentral hypergeometric, and noncentral negative hypergeometric
distributions.

\REVISED

\subsection{Package Version 0.1}

After a great struggle, a very simple algorithm was decided on for
calculating $\phi(x, \alpha, \theta)$ for the binomial distribution.
Given $\alpha$ and $\theta$,
calculate the appropriate $c_1$, $c_2$, $\gamma_1$, and $\gamma_2$
as follows.

First handle the special cases where $\alpha$ is zero or one
and $\theta$ is on the boundary of the parameter space, using
Theorem~\ref{th:umpu-end} above and the obvious fact that $\phi$ is
identically equal to one when $\alpha = 1$ and identically equal to zero
when $\alpha = 0$.

When in the general case $0 < \alpha < 1$ and $\theta$ not on the boundary
choose some $c_1$ and $c_2$ such that $c_1 \le E_\theta\{T(X)\} \le c_2$.
We pick the $c_1$ and $c_2$ that give, with randomization, an equal tailed
test, in the hope that this is close to correct.

Then we go into an infinite loop that does the following.
\begin{itemize}
\item Calculate $\gamma_1$ and $\gamma_2$ using the current guesses
for $c_1$ and $c_2$ and equations \eqref{eq:gamma-1} and \eqref{eq:gamma-2}
above.  If the results satisfy 
$0 \le \gamma_1 \le 1$ and $0 \le \gamma_2 \le 1$, then we are done
and stop the loop.
\item Otherwise, we change $c_1$ or $c_2$, the one corresponding to the
$\gamma_i$ that violates the constraints worst.  If this $\gamma_i$ is
negative, we move the $c_i$ out by one
(i.~e.\ decrease $c_1$ or increase $c_2$)
and if this $\gamma_i$ is greater than one, we move the $c_i$ in by one.
\end{itemize}
Actually we don't do an infinite loop, because we have no theorem that
says this algorithm converges, so we have a maximum iteration count
(default 10) and just give up when it is reached.  In the examples
we have done, there has been no need to increase the iteration count.

See \path{ump/src/umpubinom.c} for an example of this algorithm.
See \path{ump/tests/umpub.R} for the tests it passed.

\subsection{Package Version 0.3}

An attempt to implement the density of abstract randomized $P$-values
and test the implementation shows that
equations \eqref{eq:gamma-1} and \eqref{eq:gamma-2} are no good.
They exhibit catastrophic cancellation for small alpha.

\MOVED[block of text with several equations]

This seems to work better, but honesty compels us to admit that this formula
also is subject to catastrophic cancellation.  Perusal of the source code
reveals several ad hoc bits of code that deal with special cases
in which the code without the adhockery fails due to catastrophic cancellation
or other problems with the inexactitude of floating point arithmetic.

It is fair to say that our code is far from an elegant and provably correct
solution to this problem.  We think Algorithm~\ref{alg:lots-of-work}
would actually be better than the one we used
in all respects except that it takes time proportional to the sample size,
which was deemed unacceptable (perhaps wrongly).

\subsection{Other Distributions}

We could perhaps do Poisson and negative binomial similarly to the
way version 0.3 does the binomial, but that will not do for the other
two distributions of interest.  For the noncentral hypergeometric
we follow \citet{liao-rosen} except they do not say how to calculate
CDF, so we need to worry about that.

In fact, we think, all distributions of interest can be calculated
following \citet{liao-rosen} (whether we actually want to do that or not)
so we discuss that.

\subsubsection{Recursion}

All of the distributions of interest satisfy a recursion relation.
Define
$$
   r_\theta(x) = \frac{f_\theta(x)}{f_\theta(x - 1)}
$$

Then for the binomial exponential family of distributions
$$
   r_\theta(x) = \frac{\binom{n}{x} p^x (1 - p)^{n - x}}
   {\binom{n}{x - 1} p^{x - 1} (1 - p)^{n - x + 1}}
   =
   \frac
   {(x - 1)! \, (n - x + 1)! \, p}
   {x! \, (n - x)! \, (1 - p)}
   =
   \frac {(n - x + 1) e^\theta} {x}
$$
% check in Mathematica
% dist = BinomialDistribution[n, 1 / (1 + Exp[- theta])]
% pmf[x_] = PDF[dist, x]
% pmf[x] / pmf[x - 1]
% r[x_] = FullSimplify[%, Assumptions -> 1 <= x && x <= n ]
% for next section
% D[Log[r[x]], x]
% Simplify[%]
% - 1 / (n - x + 1) - 1 / x - %
% Simplify[%]
% for section after that
% Solve[ r[x] == 1, x ]
and for the Poisson exponential family of distributions
$$
   r_\theta(x) = \frac{\mu^x e^{- \mu} / x!}
   {\mu^{x - 1} e^{- \mu} / (x - 1)!}
   =
   \frac{\mu}{x}
   =
   \frac{e^\theta}{x}
$$
% check in Mathematica
% dist = PoissonDistribution[Exp[theta]]
% pmf[x_] = PDF[dist, x]
% pmf[x] / pmf[x - 1]
% r[x_] = FullSimplify[%, Assumptions -> 1 <= x ]
% for next section
% D[Log[r[x]], x]
% Simplify[%]
% - 1 / x - %
% Simplify[%]
% for section after that
% Solve[ r[x] == 1, x ]
and for the negative binomial exponential family of distributions
$$
   r_\theta(x)
   =
   \frac
   {\binom{r + x - 1}{x} p^r (1 - p)^x}
   {\binom{r + x - 2}{x - 1} p^r (1 - p)^{x - 1}}
   =
   \frac
   {(r + x - 1)! \, (x - 1)! \, (1 - p)}
   {(r + x - 2)! \, x!}
   =
   \frac {(r + x - 1) e^\theta} {x}
$$
% check in Mathematica
% dist = NegativeBinomialDistribution[r, 1 - Exp[theta]]
% pmf[x_] = PDF[dist, x]
% pmf[x] / pmf[x - 1]
% r[x_] = FullSimplify[%, Assumptions -> theta < 0 && 1 <= x ]
% for next section
% D[Log[r[x]], x]
% Simplify[%]
% - (r - 1) / (x (r + x - 1)) - %
% Simplify[%]
% for section after that
% Solve[ r[x] == 1, x ]
% Simplify[%]
and for the noncentral hypergeometric exponential family of distributions
\begin{multline*}
   r_\theta(x)
   =
   \frac
   {e^{x \theta} \binom{K}{x} \binom{N - K}{n - x}}
   {e^{(x - 1) \theta} \binom{K}{x - 1} \binom{N - K}{n - x + 1}}
   \\
   =
   \frac
   {(x - 1)! \, (K - x + 1)! \, (n - x + 1)! (N - K - n + x - 1)! \, e^\theta}
   {x! \, (K - x)! \, (n - x)! \, (N - K - n + x)!}
   \\
   =
   \frac
   {(K - x + 1) (n - x + 1) e^\theta}
   {x (N - K - n + x)}
\end{multline*}
% check in Mathematica
% dist = HypergeometricDistribution[n, K, N]
% pmf[x_] = PDF[dist, x]
% pmf[x] / pmf[x - 1] Exp[theta]
% r[x_] = FullSimplify[%,
%    Assumptions -> 1 <= x && n + 1 - N + K <= x && x <= K && x <= n ]
% for next section
% D[Log[r[x]], x]
% Simplify[%]
% - 1 / (K - x + 1) - 1 / (n - x + 1) - 1 / x - 1 / (N - K - n + x) - %
% Simplify[%]
% for section after that
% Solve[ r[x] == 1, x ]
% Simplify[%]
and for the noncentral negative hypergeometric
exponential family of distributions
\begin{multline*}
   r_\theta(x)
   =
   \frac
   {\binom{x + r - 1}{x} \binom{N - r - x}{K - x} e^\theta}
   {\binom{x - 1 + r - 1}{x - 1} \binom{N - r - x + 1}{K - x + 1}}
   \\
   =
   \frac
   {(x + r - 1)! \, (N - r - x)! \, (x - 1)! \, (K - x + 1)! \, e^\theta}
   {x! \, (K - x)! \, (x + r - 2)! (N - r - x + 1)!}
   \\
   =
   \frac {(x + r - 1) (K - x + 1) e^\theta} {x (N - r - x + 1)}
\end{multline*}
% check in Mathematica, not builtin
% pmf[x_] = Binomial[x + r - 1, x] Binomial[N - r - x, K - x] / Binomial[N, K]
% pmf[x] / pmf[x - 1] Exp[theta]
% r[x_] = FullSimplify[%]
% for next section
% D[Log[r[x]], x]
% Simplify[%]
% - (r - 1) / (x (x + r - 1)) - (N - K - r) / ((K - x + 1)(N - r - x + 1)) - %
% Simplify[%]
and these recursions are the fundamental idea of the method
of \citet{liao-rosen}.

\subsubsection{Unimodality}

For binomial
$$
   \frac{d}{d x} \log r_\theta(x)
   =
   - \frac{1}{n - x + 1} - \frac{1}{x}
$$
(note that $n - x \ge 0$)
so $r_\theta(x)$ is a decreasing function of $x$ and all distributions
in the family are unimodal.

For Poisson
$$
   \frac{d}{d x} \log r_\theta(x)
   =
   - \frac{1}{x}
$$
so $r_\theta(x)$ is a decreasing function of $x$ and all distributions
in the family are unimodal.

For negative binomial
$$
   \frac{d}{d x} \log r_\theta(x)
   =
   \frac{1}{r + x - 1} - \frac{1}{x}
   =
   - \frac{r - 1}{x (r + x - 1)}
$$
(note that $r \ge 1$)
so $r_\theta(x)$ is a decreasing function of $x$ and all distributions
in the family are unimodal.

For noncentral hypergeometric
$$
   \frac{d}{d x} \log r_\theta(x)
   =
   - \frac{1}{K - x + 1}
   - \frac{1}{n - x + 1}
   - \frac{1}{x}
   - \frac{1}{N - K - n + x}
$$
(note the denominator of each fraction is positive)
so $r_\theta(x)$ is a decreasing function of $x$ and all distributions
in the family are unimodal.

For noncentral negative hypergeometric
\begin{align*}
   \frac{d}{d x} \log r_\theta(x)
   & =
   \frac{1}{x + r - 1}
   - \frac{1}{K - x + 1}
   - \frac{1}{x}
   + \frac{1}{N - r - x + 1}
   \\
   & =
   - \frac{r - 1}{x (x + r - 1)}
   - \frac{N - K - r}{(K - x + 1)(N - r - x + 1)}
\end{align*}
(note that $1 \le r \le N - K$ and $1 \le x \le K$)
so $r_\theta(x)$ is a decreasing function of $x$ and all distributions
in the family are unimodal.

\subsubsection{Modes}

Thus for each distribution the largest $x$ such that $r_\theta(x) \ge 1$
is the unique mode if $r_\theta(x) > 1$ for that $x$, whereas $x$ and $x - 1$
are both modes if $r_\theta(x) = 1$ for that $x$.

To simplify notation, for all distributions,
we write $\rho = e^\theta$ and
we write $m_\theta$ for the mode.

For binomial
\begin{align*}
   r_\theta(x) & = \frac{e^\theta}{x}
   \\
   m_\theta & = \lfloor \rho \rfloor
\end{align*}
% try binomial
% n <- 23; theta <- -1
% mode <- floor((n + 1) * exp(theta) / (1 + exp(theta)))
% xx <- 0:n
% p <- 1 / (1 + exp(- theta))
% pp <- dbinom(xx, n, p)
% max(pp - dbinom(mode, n, p))

For Poisson
\begin{align*}
   r_\theta(x) & = \frac {(n - x + 1) e^\theta} {x}
   \\
   m_\theta & = \lfloor (n + 1) \rho / (1 + \rho) \rfloor
\end{align*}
% try poisson
% theta <- 2.222
% mode <- floor(exp(theta))
% xx <- 0:1000 # hopefully big enough
% mu <- exp(theta)
% pp <- dpois(xx, mu)
% max(pp - dpois(mode, mu))

For negative binomial
\begin{align*}
   r_\theta(x) & = \frac {(r + x - 1) e^\theta} {x}
   \\
   m_\theta & = \lfloor (r - 1) \rho / (1 - \rho) \rfloor
\end{align*}
% try negative binomial
% r <- 5; theta <- -1
% rho <- exp(theta)
% mode <- floor((r - 1) * rho / (1 - rho))
% xx <- 0:1000 # hopefully big enough
% p <- 1 - rho
% pp <- dnbinom(xx, r, p)
% max(pp - dnbinom(mode, r, p))

For noncentral hypergeometric
$$
   r_\theta(x)
   =
   \frac {(K - x + 1) (n - x + 1) e^\theta} {x (N - K - n + x)}
$$
so to find the mode we need to solve the quadratic equation
$$
   (K - x + 1) (n - x + 1) \rho =  x (N - K - n + x)
$$
for $x$.  This equation can be rewritten
$$
   \rho \bigl( x^2 - (K + n + 2) x + (K + 1)(n + 1) \bigr)
   =
   x^2 + (N - K - n) x
$$
% Mathematica
% (K - x + 1) (n - x + 1) rho - (x (N - K - n + x))
% Collect[%, x]
% (rho - 1) x^2 - ((K + n) (rho - 1) + N + 2 rho) x + rho (K + 1)(n + 1) - (%)
% Simplify[%]
% Solve[% == 0, x]
% simpliy discriminant
% ((K + n) (rho - 1) + N + 2 rho)^2 - 4 (rho - 1) rho (K + 1)(n + 1)
% Simplify[%]
or
$$
   (\rho - 1) x^2
   - \bigl( (K + n) (\rho - 1) + N + 2 \rho \bigr) x
   + \rho (K + 1)(n + 1)
   = 0
$$
% try R
% K <- 15; N <- 25; n <- 13; theta <- -1.111
% xmin <- max(0, n + K - N); xmax <- min(K, n)
% rho <- exp(theta)
% foo <- function(x) (K - x + 1) * (n - x + 1) * rho / (x * (N - K - n + x)) - 1
% uout <- uniroot(foo, lower = xmin, upper = xmax)
% mode <- floor(uout$root)
% xx <- seq(xmin + 1, xmax)
% rr <- 1 + foo(xx)
% pp <- c(1, cumprod(rr))
% pp <- pp / sum(pp)
% xx <- c(xmin, xx)
% data.frame(xx, pp)

\bibliographystyle{apalike}

\begin{thebibliography}{} 

\bibitem[Agresti(1992)]{agresti}
Agresti, A. (1992).
\newblock A survey of exact inference for contingency tables.
\newblock \emph{Statistical Science}, \textbf{7}, 131--153.
\newblock \doi{10.1214/ss/1177011454}.

\bibitem[Barndorff-Nielsen(1978)]{barndorff-nielsen}
Barndorff-Nielsen, O. (1978).
\newblock \emph{Information and Exponential Families}.
\newblock Wiley, Chichester.

\bibitem[Cornfield(1956)]{cornfield}
Cornfield, J. (1956).
\newblock A statistical problem arising from retrospective studies.
\newblock \emph{Proceedings of the Third Berkeley Symposium on
    Mathematical Statistics and Probability}, \textbf{4}, 135--148.
\newblock University of California, Berkeley.

\bibitem[Ferguson(1996)]{ferguson}
Ferguson, T.~S. (1996).
\newblock \emph{A Course in Large Sample Theory}.
\newblock London: Chapman \& Hall.

\bibitem[Fisher(1935)]{fisher}
Fisher, R.~A. (1935).
\newblock The logic of inductive inference (with discussion).
\newblock \emph{Journal of the Royal Statistical Society, Series A},
    \textbf{98}, 39--82.
\newblock \doi{10.2307/2342435}.

\bibitem[Geyer(1990)]{geyer-thesis}
Geyer, C.~J. (1990).
\newblock \emph{Likelihood and Exponential Families}.
\newblock PhD thesis, University of Washington.
\newblock \url{http://purl.umn.edu/56330}.

\bibitem[Geyer and Meeden(2005)]{geyer-meeden}
Geyer, C.~J. and Meeden, G.~D. (2005).
\newblock Fuzzy and randomized confidence intervals and $P$-values
    (with discussion).
\newblock \emph{Statistical Science}, \textbf{20}, 358--387.
\newblock \doi{10.1214/088342305000000340}.

\bibitem[Klir, et~al.(1997)Klir, St.~Clair, and Yuan]{fuzz}
\textsc{Klir, G.~J., St.~Clair, U.~H.} and \textsc{Yuan, B.} (1997).
\newblock \emph{Fuzzy Set Theory: Foundations and Applications}.
\newblock Prentice Hall PTR, Upper Saddle River, NJ.

\bibitem[Lehmann, 1959]{lehmann}
\textsc{Lehmann, E.~L.} (1959).
\newblock \emph{Testing Statistical Hypotheses}.
\newblock Wiley, New York.
\newblock (2nd ed., Wiley, 1986; Springer, 1997).

\bibitem[Liao and Rosen(2001)]{liao-rosen}
Liao, J.~G., and Rosen, O. (2001).
\newblock Fast and stable algorithms for computing and sampling from
    the noncentral hypergeometric distribution.
\newblock \emph{American Statistician}, \textbf{55}, 366--369.
\newblock \doi{10.1198/000313001753272547}.

\bibitem[Martin, et al.(2022)Martin, Quinn, and Park]{MCMCpack-package}
Martin, A.~D., Quinn, K.~M., Park, J.~H. (2022).
\newblock R package \code{MCMCpack}: : Markov chain Monte Carlo (MCMC) package,
    version 1.6-3.
\newblock \url{https://cran.r-project.org/package=MCMCpack}.

\bibitem[Wikipedia contributors(2021)]{noncentral-hypergeometric-distribution}
Wikipedia contributors (2021).
\newblock Fisher's noncentral hypergeometric distribution.
\newblock In \emph{Wikipedia, The Free Encyclopedia}.
\newblock \url{https://en.wikipedia.org/w/index.php?title=Fisher\%27s_noncentral_hypergeometric_distribution&oldid=1004931849}.

\bibitem[Wikipedia contributors(2023a)]{hypergeometric-distribution}
Wikipedia contributors (2023a).
\newblock Hypergeometric distribution.
\newblock In \emph{Wikipedia, The Free Encyclopedia}.
\newblock \url{https://en.wikipedia.org/w/index.php?title=Hypergeometric_distribution&oldid=1162227027}.

\bibitem[Wikipedia contributors(2023b)]{negative-hypergeometric-distribution}
Wikipedia contributors (2023b).
\newblock Negative hypergeometric distribution.
\newblock In \emph{Wikipedia, The Free Encyclopedia}.
\newblock \url{https://en.wikipedia.org/w/index.php?title=Negative_hypergeometric_distribution&oldid=1160615854}.

\end{thebibliography}

\appendix

\section{Algorithm for Fuzzy $\boldsymbol{P}$-Values}

Here we are interested in $x$ fixed at the observed data
and $\theta$ fixed at the value hypothesized under the null hypothesis.

We know from Algorithm~1 that $C_1 \le \mu \le C_2$.
Hence if $T(x) \le \mu$, we have $\phi(x, \alpha, \theta)$
strictly between 0 and 1 if and only if $C_1 = T(x)$.

Then in \eqref{eq:umpu-a-general} and \eqref{eq:umpu-b-general}
$p_1$ and $P_1$ and $M_1$ are fixed (because $C_1$ and $\theta$ are fixed),
from which we see
\begin{align*}
  P_1 + \gamma_1 p_1 & \le \alpha
  \\
  M_1 + \gamma_1 C_1 p_1 & \le \alpha \mu
\end{align*}
so
$$
   \alpha \ge \max\left(P_1, \frac{M_1}{\mu}\right)
$$

So now we need a guess about what $C_2$ may be.  Because of discreteness,
we have no way to calculate except by trial and error.  Start with an
approximately equal-tailed interval, that is,
\begin{gather*}
   P_1 \approx P_2
   \\
   P_1 + p_1 \approx p_2 + P_2
\end{gather*}
For example, we could set $C_2$ to be the $1 - P_1$ quantile.

The next step is to use \eqref{eq:gamma-1} and \eqref{eq:gamma-2}
to see if we have a solution, that is, if there is any range of $\alpha$
for which these equations both evaluate to something between 0 and 1,
that is, $\alpha$ such that
\begin{align*}
   0 \le
   \frac{\alpha (C_2 - \mu) + (M_1 - C_2 P_1) + (M_2 - C_2 P_2)}
   {p_1 (C_2 - C_1)}
   \le 1
   \\
   0 \le
   \frac{\alpha (\mu - C_1) - (M_2 - C_1 P_2) - (M_1 - C_1 P_1)}
   {p_2 (C_2 - C_1)}
   \le 1
\end{align*}
both hold.  We should put in the assumption that $C_1 < C_2$.  In
case we have $T(x) = \mu$, the fuzzy $P$-value is given by
\eqref{eq:umpu-spec-a} with $C_1 = C_2 = C = \mu$ and $p_1 = p_2 = p$.

\begin{align*}
   \frac{- (M_1 - C_2 P_1) - (M_2 - C_2 P_2)}{C_2 - \mu} \le
   \alpha
   \le \frac{p_1 (C_2 - C_1) - (M_1 - C_2 P_1) - (M_2 - C_2 P_2)}{C_2 - \mu}
   \\
   \frac{(M_2 - C_1 P_2) + (M_1 - C_1 P_1)}{\mu - C_1} \le
   \alpha
   \le \frac{p_2 (C_2 - C_1) + (M_2 - C_1 P_2) + (M_1 - C_1 P_1)}{\mu - C_1}
\end{align*}
% Mathematica
% Solve[(alpha (C2 - mu) + (M1 - C2 P1) + (M2 - C2 P2)) / (p1 (C2 - C1)) == 0,
%     alpha]
% Solve[(alpha (mu - C1) - (M2 - C1 P2) - (M1 - C1 P1)) / (p2 (C2 - C1)) == 0,
%     alpha]
% Solve[(alpha (C2 - mu) + (M1 - C2 P1) + (M2 - C2 P2)) / (p1 (C2 - C1)) == 1,
%     alpha]
% Solve[(alpha (mu - C1) - (M2 - C1 P2) - (M1 - C1 P1)) / (p2 (C2 - C1)) == 1,
%     alpha]

Try an example, Poisson, execute algorithm 1.
<<poisson-test>>=
fuzzy.pval <- function(mu) {
    stopifnot(is.numeric(mu))
    stopifnot(is.finite(mu))
    stopifnot(length(mu) == 1)
    stopifnot(mu > 0)
    C1 <- floor(mu)
    C2 <- ceiling(mu)
    save <- c(C1, C2, 1, 1, 1) |> rbind(deparse.level = 0)
    colnames(save) <- c("C1", "C2", "gamma1", "gamma2", "alpha")
    if (C1 == C2) {
        p <- ppois(C1, mu)
        alpha <- 1 - p
        gamma <- 1 - (1 - alpha) / p
        save <- rbind(save, c(C1, C2, gamma, gamma, alpha))
        C1 <- C1 - 1
        C2 <- C2 + 1
        save <- rbind(save, c(C1, C2, 1, 1, alpha))
    }
    gamma1 <- 1
    gamma2 <- 1
    repeat {
        p1 <- dpois(C1, mu)
        p2 <- dpois(C2, mu)
        P1 <- ppois(C1 - 1, mu)
        P2 <- ppois(C2, mu, lower.tail = FALSE)
        M1 <- mu * ppois(C1 - 2, mu)
        M2 <- mu * ppois(C2 - 1, mu, lower.tail = FALSE)
        L1 <- (- (M1 - C2 * P1) - (M2 - C2 * P2)) / (C2 - mu)
        L2 <- ((M2 - C1 * P2) + (M1 - C1 * P1)) / (mu - C1)
        alpha <- max(L1, L2)
        gamma1 <- (alpha * (C2 - mu) + (M1 - C2 * P1) + (M2 - C2 * P2)) /
            (p1 * (C2 - C1))
        gamma2 <- (alpha * (mu - C1) - (M2 - C1 * P2) - (M1 - C1 * P1)) /
            (p2 * (C2 - C1))
        save <- rbind(save, c(C1, C2, gamma1, gamma2, alpha))
        if (L1 > L2) {
            C1 <- C1 - 1
            gamma1 <- 1
        } else {
            C2 <- C2 + 1
            gamma2 <- 1
        }
        save <- rbind(save, c(C1, C2, gamma1, gamma2, alpha))
        if (alpha < 1e-5) break
    }
    as.data.frame(save)
}

fuzzy.pval(5.55) |> zapsmall()
fuzzy.pval(5) |> zapsmall()
@

Looks like that works.  Now we need a function that works for just one $x$.
<<poisson-test-x-fixed>>=
fuzzy.pval.too <- function(mu, x) {
    stopifnot(is.numeric(mu))
    stopifnot(is.finite(mu))
    stopifnot(length(mu) == 1)
    stopifnot(mu > 0)
    stopifnot(is.numeric(x))
    stopifnot(is.finite(x))
    stopifnot(length(x) == 1)
    stopifnot(x >= 0)
    stopifnot(x == round(x))

    # special case x == mu
    if (x == mu) {
        p <- ppois(x, mu)
        alpha <- 1 - p
        return(list(alpha = c(alpha, 1), df = c(0, 1)))
    }

    # regular case mu < x
    if (x > mu) {
        C2 <- x
        p2 <- dpois(C2, mu)
        P2 <- ppois(C2, mu, lower.tail = FALSE)
        M2 <- mu * ppois(C2 - 1, mu, lower.tail = FALSE)
        # guess C1
        C1 <- qpois(P2, mu)
        p1 <- dpois(C1, mu)
        P1 <- ppois(C1 - 1, mu)
        M1 <- mu * ppois(C1 - 2, mu)
        L1 <- (- (M1 - C2 * P1) - (M2 - C2 * P2)) / (C2 - mu)
        L2 <- ((M2 - C1 * P2) + (M1 - C1 * P1)) / (mu - C1)
        U1 <- (p1 * (C2 - C1) - (M1 - C2 * P1) - (M2 - C2 * P2)) / (C2 - mu)
        U2 <- (p2 * (C2 - C1) + (M2 - C1 * P2) + (M1 - C1 * P1)) / (mu - C1)
        alpha <- c(max(L1, L2), min(U1, U2))
        gamma2 <- (alpha * (mu - C1) - (M2 - C1 * P2) - (M1 - C1 * P1)) /
            (p2 * (C2 - C1))
        gamma2 <- zapsmall(gamma2)
        return(list(alpha = alpha, df = gamma2))
    }
}

fuzzy.pval.too(5, 5)
fuzzy.pval(5.55) |> subset(C2 == 8) |> zapsmall()
fuzzy.pval.too(5.55, 8)
@

\section{Fuzzy Confidence Interval Examples} \label{sec:fci-plots}

First example (Figure~\ref{fig:fuzzy-confint-one} below),
note that when observed $x$ is as low as possible (zero for binomial),
the fuzzy confidence interval starts at $1 - \alpha$ as per
Theorem~\ref{th:umpu-end}.
Also note that in this case the core (set where membership function is equal
to one) is empty.

<<fuzzy-confint-one, fig.align="center", fig.cap = "Binomial Fuzzy Confidence Interval, x = 0, n = 10, alpha = 0.05">>=
library(ump)
p <- seq(0, 1, length = 1001)
phi <- umpu.binom(0, 10, p, 0.05)
xmax <- min(p[phi == 1])
xmax <- 1.1 * xmax
plot(p, 1 - phi, xlim = c(0, xmax), ylim = 0:1,
    xlab = "success probability", ylab = "", type = "l")
@

Second example (Figure~\ref{fig:fuzzy-confint-two} below),
note that when observed $x$ is adjacent to its minimum value (one for binomial),
the fuzzy confidence interval starts at $1 - \alpha$ as per
Theorem~\ref{th:umpu-end}
and goes up from there until it reaches one, and is nonincreasing thereafter.
In this example, the core is nonempty.

<<fuzzy-confint-two, fig.align="center", fig.cap = "Binomial Fuzzy Confidence Interval, x = 1, n = 10, alpha = 0.05">>=
phi <- umpu.binom(1, 10, p, 0.05)
xmax <- min(p[phi == 1])
xmax <- 1.1 * xmax
plot(p, 1 - phi, xlim = c(0, xmax), ylim = 0:1,
    xlab = "success probability", ylab = "", type = "l")
@

Third example (Figure~\ref{fig:fuzzy-confint-three} below),
another one just like the previous one except that the coverage
probability is less, so the behavior is more extreme.

<<fuzzy-confint-three, fig.align="center", fig.cap = "Binomial Fuzzy Confidence Interval, x = 1, n = 10, alpha = 0.5">>=
phi <- umpu.binom(1, 10, p, 0.5)
xmax <- min(p[phi == 1])
xmax <- 1.1 * xmax
plot(p, 1 - phi, xlim = c(0, xmax), ylim = 0:1,
    xlab = "success probability", ylab = "", type = "l")
@

Fourth example (Figure~\ref{fig:fuzzy-confint-four} below),
another one just like the previous one except that the coverage
probability is less again, so the behavior is even more extreme.  For this one
the coverage probability requested is so low that the core is empty.
In order to have exactly 25\% confidence, the membership function cannot
go up to one.  The membership function reaches its maximum value at $\mu = x$
as per Theorem~\ref{th:fuzzy-convex}.

<<fuzzy-confint-four, fig.align="center", fig.cap = "Binomial Fuzzy Confidence Interval, x = 1, n = 10, alpha = 0.75">>=
phi <- umpu.binom(1, 10, p, 0.75)
xmax <- min(p[phi == 1])
xmax <- 1.1 * xmax
plot(p, 1 - phi, xlim = c(0, xmax), ylim = 0:1,
    xlab = "success probability", ylab = "", type = "l")
@

Fifth example (Figure~\ref{fig:fuzzy-confint-five} below),
now the observed $x$ is not either of the two lowest possible values
or either of the two highest.  So the fuzzy confidence interval has the
typical behavior: as $\mu$ and $\theta$ increase from one end of the
parameter space to the other, the fuzzy confidence interval is zero
for a while, then increases smoothly to one, then is one for a while
(the core), then decreases smoothly to zero, then is zero the rest of the
way.

<<fuzzy-confint-five, fig.align="center", fig.cap = "Binomial Fuzzy Confidence Interval, x = 4, n = 10, alpha = 0.05">>=
phi <- umpu.binom(4, 10, p, 0.05)
xmax <- max(p[phi < 1])
xmin <- min(p[phi < 1])
xmax <- 1.1 * xmax
xmin <- xmin / 1.1
plot(p, 1 - phi, xlim = c(xmin, xmax), ylim = 0:1,
    xlab = "success probability", ylab = "", type = "l")
@

Sixth example (Figure~\ref{fig:fuzzy-confint-six} below),
another one just like the previous one except that the coverage
probability is less, so the behavior is more extreme.

<<fuzzy-confint-six, fig.align="center", fig.cap = "Binomial Fuzzy Confidence Interval, x = 4, n = 10, alpha = 0.5">>=
phi <- umpu.binom(4, 10, p, 0.5)
xmax <- max(p[phi < 1])
xmin <- min(p[phi < 1])
xmax <- 1.1 * xmax
xmin <- xmin / 1.1
plot(p, 1 - phi, xlim = c(xmin, xmax), ylim = 0:1,
    xlab = "success probability", ylab = "", type = "l")
@

Seventh example (Figure~\ref{fig:fuzzy-confint-seven} below),
another one just like the previous one except that the coverage
probability is less again, so the behavior is even more extreme.  For this one
the coverage probability requested is so low that the core is empty.
In order to have exactly 20\% confidence, the membership function cannot
go up to one.
The membership function reaches its maximum value at $\mu = x$
as per Theorem~\ref{th:fuzzy-convex}.

<<fuzzy-confint-seven, fig.align="center", fig.cap = "Binomial Fuzzy Confidence Interval, x = 4, n = 10, alpha = 0.8">>=
phi <- umpu.binom(4, 10, p, 0.8)
xmax <- max(p[phi < 1])
xmin <- min(p[phi < 1])
xmax <- 1.1 * xmax
xmin <- xmin / 1.1
plot(p, 1 - phi, xlim = c(xmin, xmax), ylim = 0:1,
    xlab = "success probability", ylab = "", type = "l")
@



\end{document}



\section{Computing}

\subsection{Nice Models}

In this section we take another shot at ``nice models'' by which we mean
those for which R has functions for calculating most things about the
distribution.  Of the models mentioned above, the nice models are the
binomial, Poisson, and negative binomial.  The exponential families
generated by the hypergeometric and negative hypergeometric distributions
are not ``nice.''

Let $F_\theta$ and $F_\theta^{-1}$ denote the cumulative distribution
and quantile functions as implemented in R (for example, \texttt{plenum}
and \texttt{qbinom})
$$
   F_\theta^{-1}(p) = \inf \set{ x : F_\theta(x) \ge p }
$$
Define also an analogous ``$M$'' function for the mean
$$
   M_\theta(x) = \sum_{y = 0}^x y f_\theta(y)
$$
and its inverse
$$
   M_\theta^{-1}(s) = \inf \set{ x : M_\theta(x) \ge s }
$$
As discussed in the sections on the models, these ``$M$'' functions are
implemented in R as follows
\begin{verbatim}
mbinom <- function(x, n, p)
    n * p * pbinom(x - 1, n - 1, p)
mpois <- function(x, mu)
    mu * ppois(x - 1, mu)
mnbinom <- function(x, r, p)
    (1 - p) / p * pnbinom(x - 1, r + 1, p)
\end{verbatim}
The corresponding inverse functions are
\begin{verbatim}
nbinom <- function(s, n, p)
    1 + qbinom(s / (n * p), n - 1, p)
npois <- function(s, mu)
    1 + qpois(s / mu, mu)
nnbinom <- function(s, r, p)
    1 + qnbinom(s * p / (1 - p), r + 1, p)
\end{verbatim}

Then we can rewrite write the key
equations \eqref{eq:gamma-1} and \eqref{eq:gamma-2} as
\begin{align*}
   1 - \gamma_1
   & =
   \frac{(1 - \alpha) (C_2 - \mu) + m_{1 2} - C_2 p_{1 2}}
   {f_\theta(C_1) (C_2 - C_1)}
   \\
   1 - \gamma_2
   & =
   \frac{(1 - \alpha) (\mu - C_1) - m_{1 2} + C_1 p_{1 2}}
   {f_\theta(C_2) (C_2 - C_1)}
\end{align*}
where
\begin{align*}
   p_{1 2} & = F_\theta(C_2 - 1) - F_\theta(C_1)
   \\
   m_{1 2} & = M_\theta(C_2 - 1) - M_\theta(C_1)
\end{align*}
In order to have the gammas in range we need to satisfy the inequalities
\begin{align*}
   0
   & \le
   \frac{(1 - \alpha) (C_2 - \mu) + m_{1 2} - C_2 p_{1 2}}
   {f_\theta(C_1) (C_2 - C_1)}
   \le
   1
   \\
   0
   & \le
   \frac{(1 - \alpha) (\mu - C_1) - m_{1 2} + C_1 p_{1 2}}
   {f_\theta(C_2) (C_2 - C_1)}
   \le
   1
\end{align*}
or
\begin{gather*}
   0
   \le
   (1 - \alpha) (C_2 - \mu) + m_{1 2} - C_2 p_{1 2}
   \le
   f_\theta(C_1) (C_2 - C_1)
   \\
   0
   \le
   (1 - \alpha) (\mu - C_1) - m_{1 2} + C_1 p_{1 2}
   \le
   f_\theta(C_2) (C_2 - C_1)
\end{gather*}
Adding these inequalities to eliminate $m_{1 2}$ gives
$$
   0 \le (1 - \alpha - p_{1 2}) (C_2 - C_1)
   \le \bigl[ f_\theta(C_1) + f_\theta(C_2) \bigr] (C_2 - C_1)
$$
or
$$
   0 \le 1 - \alpha - F_\theta(C_2 - 1) + F_\theta(C_1)
   \le f_\theta(C_1) + f_\theta(C_2)
$$
or
\pagebreak[3]
\begin{subequations}
\begin{equation} \label{eq:fred-a}
   F_\theta(C_2 - 1) - F_\theta(C_1)
   \le
   1 - \alpha
   \le
   F_\theta(C_2) - F_\theta(C_1 - 1)
\end{equation}
Similarly, eliminating $p_{1 2}$ gives
\begin{equation} \label{eq:fred-b}
   M_\theta(C_2 - 1) - M_\theta(C_1)
   \le
   (1 - \alpha) \mu
   \le
   M_\theta(C_2) - M_\theta(C_1 - 1)
\end{equation}
\end{subequations}
Note that in hindsight \eqref{eq:fred-a} and \eqref{eq:fred-b} are obvious.

In order to satisfy \eqref{eq:fred-a} and \eqref{eq:fred-b} we must have
$$
   F_\theta^{-1}\bigl\{ F_\theta(C_1 - 1) + 1 - \alpha \bigr\}
   \le
   C_2
   \le
   1 + F_\theta^{-1}\bigl\{ F_\theta(C_1) + 1 - \alpha + \epsilon \bigr\},
   \qquad \forall \epsilon > 0
$$
and
$$
   M_\theta^{-1}\bigl\{ M_\theta(C_1 - 1) + (1 - \alpha) \mu \bigr\}
   \le
   C_2
   \le
   1 + M_\theta^{-1}\bigl\{ M_\theta(C_1) + (1 - \alpha) \mu + \epsilon \bigr\},
   \qquad \forall \epsilon > 0
$$
the $\forall \epsilon > 0$ being an unavoidable hassle caused by the
interaction of discreteness and the right continuity convention.
It's too bad the R ``q'' functions don't come with a flag
for either right or left continuity.
Basically, these are just \eqref{eq:fred-a} and \eqref{eq:fred-b} ``inverted''
which is what we need to use in the computer, but we will stick with the
simpler \eqref{eq:fred-a} and \eqref{eq:fred-b} for analysis.

Now suppose we are interested in one fixed $x$ and $\theta$.  Suppose,
for concreteness that $x < \mu$ so if $\phi(x, \alpha, \theta)$ is
strictly between zero and one we must have $x = C_1$.  Then our inequalities
can be used to determine the range of $\alpha$ and $C_2$ for which we
get solutions.

For starters, we must have $\mu < C_2 \le x_{\text{max}}$, where 
$x_{\text{max}}$ is the upper bound of the support (perhaps $\infty$).
And this implies from \eqref{eq:fred-a} that
$$
   F_\theta(C_1 - 1) \le \alpha \le 1 - F_\theta(\mu) + F_\theta(C_1)
$$
and from \eqref{eq:fred-b} that
$$
   \frac{M_\theta(C_1 - 1)}{\mu}
   \le
   \alpha
   \le
   1 - \frac{M_\theta(\mu) - M_\theta(C_1)}{\mu}
$$

\section{New Computing}

In this section (November, 2002) we look at the analog of following
the curve $\theta \mapsto \phi(x, \alpha, \theta)$ to get
the membership function of a fuzzy confidence interval, the same
way we followed $\alpha \mapsto \phi(x, \alpha, \theta)$ to get the
cumulative distribution function of a fuzzy $P$-value.
The latter is piecewise linear.  The former isn't.  But how hard is it?

We know how to get started from the ``endpoint'' analysis
(Section~\ref{sec:endpoint}).  The membership function of the fuzzy
confidence interval goes to $1 - \alpha$ or to 0 as $\theta$ goes to
its upper or lower limit, depending on the value of $x$.
(This means we are still in ``nice'' models, one-parameter exponential
families with canonical statistic taking values on a range of consecutive
integers).

So we can start at the edge of the parameter space and go from there.
What do equations \eqref{eq:gamma-1} and \eqref{eq:gamma-2} do
\emph{considered as functions of $\theta$ for fixed $\alpha$}?

Let's start with the case of $\theta$ near the boundary.  Suppose
the sample space of the canonical statistic is $\{ 0, 1, \ldots \}$
(so we are in the situation
analyzed in Section~\ref{sec:endpoint}, but we want to know more than
just $o(\mu)$ used there.  For small enough $\theta$ we do have $C_1 = 0$
and $C_2 = 1$ so $p_{1 2} = m_{1 2} = 0$ and
equations \eqref{eq:gamma-1} and \eqref{eq:gamma-2} become
\begin{subequations}
\begin{align}
   1 - \gamma_1
   & =
   \frac{(1 - \alpha) (1 - E_\theta\{T(X)\})}{\pr_\theta\{T(X) = 0\}}
   \label{eq:gamma-1-mbogo}
   \\
   1 - \gamma_2
   & =
   \frac{(1 - \alpha) E_\theta\{T(X)\}}{\pr_\theta\{T(X) = 1\}}
   \label{eq:gamma-2-mbogo}
\end{align}
\end{subequations}

For the $\text{Binomial}(n, p)$ distribution these become
\begin{subequations}
\begin{align}
   1 - \gamma_1
   & =
   \frac{(1 - \alpha) (1 - n p)}{(1 - p)^n}
   \label{eq:gamma-1-mbogo-bino}
   \\
   1 - \gamma_2
   & =
   \frac{(1 - \alpha) n p}{n p (1 - p)^{n - 1}}
   \nonumber
   \\
   & =
   \frac{(1 - \alpha)}{(1 - p)^{n - 1}}
   \label{eq:gamma-2-mbogo-bino}
\end{align}
\end{subequations}
So we see these equations, though nonlinear, are not horribly intractable.
They are good for the $\theta$ for which their values stay between zero
and one.  For \eqref{eq:gamma-2-mbogo-bino} this is for
$$
   0 \le p \le 1 - (1 - \alpha)^{1 / (n - 1)}
$$
at the upper end of this range where $\gamma_2 = 1$, we can make the
change $C_2 = 2$ and $\gamma_2 = 0$ without affecting the critical function.

With either of the $C$'s changed we get the more general equations
\eqref{eq:gamma-1} and \eqref{eq:gamma-2} now in the form
\begin{subequations}
\begin{align}
   1 - \gamma_1
   & =
   \frac{(1 - \alpha) [C_2 - \mu(\theta)] - E_\theta\{I_{1 2}(X) [C_2 - X]\}}
   {f_\theta(C_1) (C_2 - C_1)}
   \label{eq:gamma-1-muggs}
   \\
   1 - \gamma_2
   & =
   \frac{(1 - \alpha) [\mu(\theta) - C_1] - E_\theta\{I_{1 2}(X) [X - C_1]\}}
   {f_\theta(C_2) (C_2 - C_1)}
   \label{eq:gamma-2-muggs}
\end{align}
\end{subequations}
where
$$
   I_{1 2}(x) = \begin{cases} 1, & C_1 < x < C_2 \\
   0, & \text{otherwise} \end{cases}
$$
It follows from the general properties of exponential families that
if $\theta$ is the canonical parameter
$$
   \frac{\partial}{\partial \theta} f_\theta(x) = [x - \mu(\theta)] f_\theta(x)
$$
$$
   \frac{\partial}{\partial \theta} \mu(\theta) = \sigma^2(\theta)
$$
Hence
\begin{align*}
   \frac{\partial}{\partial \theta} (1 - \gamma_1)
   & =
   \frac{1}{f_\theta(C_1) (C_2 - C_1)} \times \biggl(
   - (1 - \alpha) \sigma^2(\theta)
   \\
   & \qquad
   - E_\theta\{I_{1 2}(X) [C_2 - X] [X - \mu(\theta)]\}
   \\
   & \qquad
   + (1 - \alpha) [C_2 - \mu(\theta)] [\mu(\theta) - C_1]
   \\
   & \qquad
   - E_\theta\{I_{1 2}(X) [C_2 - X] [\mu(\theta) - C_1]\} \biggr)
   \\
   & =
   \frac{1}{f_\theta(C_1) (C_2 - C_1)} \times \biggl(
\end{align*}




In considering the distribution of the fuzzy $P$-value, we look at
$\phi(x, \alpha, \theta)$ as a function of $\alpha$ for fixed $x$ and $\theta$,
hence at \eqref{eq:gamma-ump} in the same way.  Now $T(x)$ will be a
$(1 - \alpha)$-th quantile if
$$
   \pr_\theta\{ T(X) > T(x) \} \le \alpha \le 
   \pr_\theta\{ T(X) \ge T(x) \}.
$$
Hence
\begin{align*}
   \phi(x, \alpha, \theta)
   & =
   \begin{cases}
   0, & \pr_\theta\{ T(X) \ge T(x) \} \ge \alpha
   \\
   1, & \pr_\theta\{ T(X) > T(x) \} \le \alpha
   \\
   \frac{\alpha - \pr_\theta\{T(X) > T(x)\}}{\pr_\theta\{T(X) = T(x)\}}, &
   \text{otherwise}
   \end{cases}
\end{align*}
Since this is linear where not zero or one,
it is the distribution function of a uniform random variable
\begin{equation} \label{eq:fpv-ump-dist}
   P \sim \text{Uniform}\bigl( \pr_\theta\{ T(X) > T(x) \},
   \pr_\theta\{ T(X) \ge T(x) \} \bigr),
\end{equation}
that is, the fuzzy $P$-value is uniformly distributed on the interval
of values that you figure it should spread it over.  Intuitively obvious, but
we had to check to be sure.  Please don't feel put upon by our belaboring
the obvious, because the UMPU two-tailed case is not quite so obvious, and
understanding the UMP one-tailed case helps.

\end{document}
